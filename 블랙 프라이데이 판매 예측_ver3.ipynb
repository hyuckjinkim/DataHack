{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1478bdb",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "># **코드설명**\n",
    "\n",
    "---\n",
    "\n",
    "- 파 일 명 : 블랙 프라이데이 판매 예측 <br>\n",
    "- 시작날짜 : 2021.11.25 <br>\n",
    "- 수정날짜 : 2022.01.07 <br>\n",
    "- 작 성 자 : 김혁진 <br>\n",
    "- 작성주제 : DataHackaton / 블랙 프라이데이 판매 예측 <br>\n",
    "\n",
    "--- \n",
    "\n",
    "- **참조**\n",
    "\n",
    "  (1) 대회 홈페이지 : [DataHackaton](https://datahack.analyticsvidhya.com/contest/black-friday/#About) <br>\n",
    "  (2) 하이퍼 파리미터 설명 : [Naver Blog](https://blog.naver.com/wideeyed/221333529176) <br>\n",
    "  (3) Class문 설명 : [Github](https://zzsza.github.io/development/2020/07/05/python-class/) <br>\n",
    "  (4) GPU 설정 : [Medium](https://medium.com/@am.sharma/lgbm-on-colab-with-gpu-c1c09e83f2af) <br>\n",
    "  (5) RAM 모두사용으로 세션다운 : [Tistory](https://somjang.tistory.com/entry/Google-Colab-%EC%9E%90%EC%A3%BC%EB%81%8A%EA%B8%B0%EB%8A%94-%EB%9F%B0%ED%83%80%EC%9E%84-%EB%B0%A9%EC%A7%80%ED%95%98%EA%B8%B0)\n",
    "\n",
    "---\n",
    "\n",
    "- **고려사항** <br>\n",
    "  (1) AutoEncoder로 파생변수 생성해보기 <br>\n",
    "  (2) 하이퍼파라미터 탐색 : grid-search, bayesian-optimization, [optuna](https://dacon.io/competitions/official/235713/codeshare/2704?page=1&dtype=recent) <br>\n",
    "  (3) RandomForest, XGBoost, Lightgbm, CatBoost 설명 [블로그](https://jhkim0759.tistory.com/12)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8eb9c1",
   "metadata": {},
   "source": [
    "># **기본설정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee96f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "git config --global user.email \"hyuckjin23@gmail.com\"\n",
    "git config --global user.name  \"hyuckjinkim\"\n",
    "git config -l\n",
    "\n",
    "cd 'C:/Users/My/★PSC/1. 블랙 프라이데이 판매 예측(데이터 해커톤)'\n",
    "# mkdir workplace\n",
    "# cd workplace\n",
    "git init\n",
    "\n",
    "# echo \"# first-repository\" >> README.md\n",
    "# cat README.md\n",
    "\n",
    "git status\n",
    "\n",
    "git 1. 블랙 프라이데이 판매 예측_ver3.ipynb\n",
    "git commit -m “new readme file”\n",
    "\n",
    "cd 'C:/Users/My/★PSC/1. 블랙 프라이데이 판매 예측(데이터 해커톤)'\n",
    "git remote add origin https://github.com/hyuckjinkim/DataHackaton\n",
    "    \n",
    "git push origin master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b59740e",
   "metadata": {},
   "source": [
    "## Query Start Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248d1e03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:17.474120Z",
     "start_time": "2022-01-08T06:27:17.470620Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "query_start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c2214",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Markdown : Tabular Left Align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a624da7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:17.493623Z",
     "start_time": "2022-01-08T06:27:17.480121Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f52129",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Jupyter Notebook Style : Theme, Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "845b639a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:51:38.746678Z",
     "start_time": "2022-01-08T06:51:38.742178Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # theme 설치\n",
    "# !pip install jupyterthemes\n",
    "\n",
    "# # jupyter notebook 최신버전\n",
    "# !pip install --upgrade notebook\n",
    "\n",
    "# # jupyter notebook 최신버전\n",
    "# !pip install --upgrade jupyterthemes\n",
    "\n",
    "# 2.2.1. 테마바꾸기(customizing)\n",
    "# !jt -t onedork -fs 115 -nfs 125 -tfs 115 -dfs 115 -ofs 115 -cursc r -cellw 80% -lineh 115 -altmd  -kl -T -N\n",
    "\n",
    "# 2.2.2. 쥬피터 노트북 화면 넓게 사용\n",
    "# 출처: https://taehooh.tistory.com/entry/Jupyter-Notebook-주피터노트북-화면-넓게-쓰는방법\n",
    "from IPython.core.display import display, HTML \n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "# # 2.2.3. 좌측 TOC 만들기\n",
    "# # 출처 : https://gmnam.tistory.com/246\n",
    "# !pip install jupyter_nbextensions_configurator\n",
    "# !pip install jupyter_contrib_nbextensions\n",
    "\n",
    "# !jupyter nbextensions_configurator enable --user\n",
    "# !jupyter contrib nbextension install --user\n",
    "\n",
    "# !pip install nbconvert\n",
    "\n",
    "# !pip install jupytext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cecfd88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:17.524126Z",
     "start_time": "2022-01-08T06:27:17.512625Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 2.3.1 Google Drive Mount\n",
    "# # (Google Drive 사용 시 설정)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount = True) # 새로운 창에서 key 를 받아서 입력해야합니다. \n",
    "\n",
    "# # 2.3.2. 메모리 에러\n",
    "# https://growingsaja.tistory.com/477"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b8cbdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:17.536128Z",
     "start_time": "2022-01-08T06:27:17.526127Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 2.3.3. GPU 사용 (6분)\n",
    "# !git clone --recursive https://github.com/Microsoft/LightGBM\n",
    "# !mkdir build\n",
    "# %cd /content/LightGBM\n",
    "# !cmake -DUSE_GPU=1 #avoid ..\n",
    "# !make -j$(nproc)\n",
    "# !sudo apt-get -y install python-pip\n",
    "# !sudo -H pip install setuptools pandas numpy scipy scikit-learn -U\n",
    "# %cd /content/LightGBM/python-package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196e8d53",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### Install Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c6112",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:17.550130Z",
     "start_time": "2022-01-08T06:27:17.537628Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip uninstall pandas -y\n",
    "# !pip uninstall numpy  -y\n",
    "# !pip uninstall lightgbm -y\n",
    "\n",
    "# !pip install pandas==1.1.0\n",
    "# !pip install numpy==1.21.2\n",
    "# !pip install -U scikit-learn\n",
    "# !pip install lightgbm --install-option=--gpu\n",
    "\n",
    "# !pip install pandasql\n",
    "# !pip install seaborn\n",
    "# !pip install plotnine\n",
    "# !pip install pandasql\n",
    "\n",
    "# lightgbm 에러떴는데, 콘다에서 실행하면 해결됨\n",
    "# conda install -c conda-forge lightgbm \n",
    "\n",
    "# bayesian optimization 설치\n",
    "# !pip install bayesian-optimization\n",
    "\n",
    "# xgboost 설치\n",
    "# !pip install xgboost\n",
    "\n",
    "# catboost 설치\n",
    "# !pip install catboost\n",
    "\n",
    "# !pip install dill\n",
    "\n",
    "# pycaret 에러떴는데, --user 붙이니깐 해결됨\n",
    "# !pip install --user pycaret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a02e69",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c6ebbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:27.988955Z",
     "start_time": "2022-01-08T06:27:17.553130Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# jupyter notebook 전용\n",
    "from tqdm.notebook import tqdm\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# basic modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "# value_counts() 범용적인 버전\n",
    "from collections import Counter as cnt\n",
    "\n",
    "# save env.\n",
    "import dill\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7, 8.27)})\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [11.7, 8.27] # [15, 10] # [11.7,8.27] - A4 size\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "from plotnine import *\n",
    "\n",
    "\n",
    "# sqldf\n",
    "from pandasql import sqldf\n",
    "sql = lambda q: sqldf(q, globals())\n",
    "\n",
    "\n",
    "# modeling\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import cross_val_score,StratifiedKFold,train_test_split\n",
    "from sklearn.metrics import f1_score,make_scorer,r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, RidgeClassifier, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor, AdaBoostClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# import lightgbm\n",
    "# !pip install lightgbm --install-option=--gpu --install-option=\"--opencl-include-dir=/usr/local/cuda/include/\" --install-option=\"--opencl-library=/usr/local/cuda/lib64/libOpenCL.so\"\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0099b6ae",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Initial Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47db36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:27.995456Z",
     "start_time": "2022-01-08T06:27:27.990455Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2.5.1. Data Path\n",
    "# jupyter.notebook : 'os.getcwd() + '/DAT/블랙 프라이데이 판매 예측/''\n",
    "# google.colab     : '/content/drive/MyDrive/Python/4. 블랙프라이데이 판매예측/DAT/'\n",
    "DATA_PATH = os.getcwd() + '/DAT/1. 블랙 프라이데이 판매 예측(데이터 해커톤)/'\n",
    "OUT_PATH  = os.getcwd() + '/OUT/1. 블랙 프라이데이 판매 예측(데이터 해커톤)/'\n",
    "\n",
    "# 2.5.2. set seed\n",
    "SEED = 777\n",
    "\n",
    "# 2.5.3. plot\n",
    "PLOT = True\n",
    "\n",
    "# 2.5.5. missing check\n",
    "MISSING_CHECK = True\n",
    "\n",
    "# 2.5.6. interaction\n",
    "INTERACTION_CHECK = True\n",
    "INTERACTION = True\n",
    "\n",
    "# 2.5.7. scaling\n",
    "SCALE_CHECK = True\n",
    "SCALE = True\n",
    "\n",
    "# 2.5.8. lightgbm parameter\n",
    "# 처음 (INIT_POINTS)회 랜덤 값으로 score 계산 후 (N_ITER)회 최적화\n",
    "INIT_POINTS = 15\n",
    "N_ITER = 15\n",
    "N_CV = 4\n",
    "EARLY_STOPPING_ROUNDS = 30\n",
    "N_ESTIMATORS = 2000\n",
    "OBJECTIVE = ['binary','regression'][1]\n",
    "METRIC = ['auc','binary_logloss','rmse'][2]\n",
    "\n",
    "# initial value save\n",
    "ini_var = [\n",
    "    'SEED','PLOT','SCALE','INTERACTION','MISSING_CHECK','INTERACTION_CHECK',\n",
    "    'INIT_POINTS','N_ITER','N_CV','EARLY_STOPPING_ROUNDS','N_ESTIMATORS','OBJECTIVE','METRIC'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd87120",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Set Off the Warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a3fc6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:28.013958Z",
     "start_time": "2022-01-08T06:27:27.997456Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb769937",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## User Defined Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a3e77b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:28.071966Z",
     "start_time": "2022-01-08T06:27:28.015459Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.1. Seed Fix\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def seed_everything(seed: int = 1):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    # torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    # torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "    \n",
    "seed_everything(SEED)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.2. View all columns\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def View(data):\n",
    "\n",
    "    pd.set_option('display.max_rows', 500)\n",
    "    pd.set_option('display.max_columns', 500)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    \n",
    "    print(data)\n",
    "\n",
    "    pd.set_option('display.max_rows', 0)\n",
    "    pd.set_option('display.max_columns', 0)\n",
    "    pd.set_option('display.width', 0)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.3. minmax function\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def minmax(x, digit=None):\n",
    "    if round is None:\n",
    "        return min(x),max(x)\n",
    "    else:\n",
    "        return round(min(x),digit),round(max(x),digit)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.4. 컬럼dict에서 target 제거\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# - dict : 기준 dict\n",
    "# - key  : 삭제할 key\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def rmkey(dict, key):\n",
    "    tmp = dict.copy()\n",
    "    del tmp[key]\n",
    "    return tmp\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.5. 각 컬럼의 missing 개수를 파악하는 함수\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# - data     : 기준 data\n",
    "# - col_type : {column명 : type}로 이루어진 dictionary\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def missing_column_check(df, col_type):\n",
    "    \n",
    "    data = df.copy()\n",
    "    \n",
    "    n_na = []\n",
    "    n_na_type = []\n",
    "    for col_nm in data.columns:\n",
    "        data[col_nm] = data[col_nm].astype(col_type[col_nm])\n",
    "\n",
    "        # str인 경우에는 blank(공백)도 있는지 확인\n",
    "        if col_type[col_nm]==str:\n",
    "\n",
    "            isnull_cnt = data[col_nm].str.strip().isnull().sum()\n",
    "            blank_cnt  = sum(data[col_nm].str.strip()=='')\n",
    "            nan_cnt    = sum(data[col_nm].str.strip()=='nan')\n",
    "            null_cnt   = sum(data[col_nm].str.strip()=='null')\n",
    "\n",
    "            n_na_x = isnull_cnt+blank_cnt+nan_cnt+null_cnt\n",
    "            n_na.append(n_na_x)\n",
    "            \n",
    "            if n_na_x>0:\n",
    "                n_na_type_x=[]\n",
    "                if isnull_cnt>0: n_na_type_x.append('isnull')\n",
    "                if blank_cnt >0: n_na_type_x.append('blank')\n",
    "                if nan_cnt   >0: n_na_type_x.append('nan')\n",
    "                if null_cnt  >0: n_na_type_x.append('null')\n",
    "                n_na_type_x = '+'.join(n_na_type_x)\n",
    "            else:\n",
    "                n_na_type_x = ''\n",
    "            n_na_type.append(n_na_type_x)\n",
    "            \n",
    "\n",
    "        # numeric인 경우에는 null의 개수만 확인\n",
    "        else:\n",
    "            n_na_x = data[col_nm].isnull().sum()\n",
    "            n_na.append(n_na_x)\n",
    "            \n",
    "            if n_na_x>0:\n",
    "                n_na_type.append('isnull')\n",
    "            else:\n",
    "                n_na_type.append('')\n",
    "            \n",
    "    res_df = pd.DataFrame({\n",
    "        'col'  : data.columns,\n",
    "        'n_na' : n_na,\n",
    "        'n_n_ratio' : [str(round(n/len(data)*100,1))+'%' for n in n_na],\n",
    "        'na_type' : n_na_type,\n",
    "        'col_type' : [COL_TYPE[col].__name__ for col in data.columns]\n",
    "        })\n",
    "\n",
    "    res_df = res_df[res_df['n_na']>0]\n",
    "    if len(res_df)==0:\n",
    "        return('Dataset does not have a null value')\n",
    "    else:\n",
    "        return(res_df)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.6. 교호작용항 추가\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# - data     : 기준 data\n",
    "# - num_vari : 숫자형 변수 list\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def interaction_term(df,num_vari):\n",
    "    \n",
    "    data = df.copy()\n",
    "\n",
    "    num_var = list(set(num_vari) - set(['id']))\n",
    "\n",
    "    for i in range(0,len(num_var)):\n",
    "        for j in range(i,len(num_var)):\n",
    "            data[f'{num_var[i]}*{num_var[j]}'] = data[f'{num_var[i]}']*data[f'{num_var[j]}']\n",
    "\n",
    "    return(data)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.7. color when print\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "class color:\n",
    "    PURPLE    = '\\033[95m'\n",
    "    CYAN      = '\\033[96m'\n",
    "    DARKCYAN  = '\\033[36m'\n",
    "    BLUE      = '\\033[94m'\n",
    "    GREEN     = '\\033[92m'\n",
    "    YELLOW    = '\\033[93m'\n",
    "    RED       = '\\033[91m'\n",
    "    BOLD      = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END       = '\\033[0m'\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.8. density plot : histogram + density plot\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# - data : 기준 data\n",
    "# - vars : hist + kde를 그릴 숫자형 변수\n",
    "# - hue  : group화 변수\n",
    "# - binwidth_adj_ratio : binwidth 조정 비율\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def density_plot(df, vars, \n",
    "                 binwidths = None, hue = None,\n",
    "                 binwidth_adj_ratio = None,\n",
    "                 figsize = (15,7)):\n",
    "\n",
    "    from matplotlib.ticker import PercentFormatter\n",
    "    \n",
    "    data = df.copy()\n",
    "\n",
    "    # 1) vars가 1개뿐일 때 에러발생\n",
    "    #    -> 1개     : type = str\n",
    "    #    -> 2개이상 : type = ndarray, ...\n",
    "    if type(vars)==str:\n",
    "        vars = [vars]\n",
    "    \n",
    "    # 2) plotting (nrow,ncol) 설정\n",
    "    nrow = math.ceil(len(vars)**(1/2))\n",
    "    ncol = nrow\n",
    "\n",
    "    # 3) binwidths가 없을 때, binwidth 설정\n",
    "    # 출처 : http://www.aistudy.co.kr/paper/pdf/histogram_jeon.pdf\n",
    "    if binwidths is None:\n",
    "        binwidths = []\n",
    "        for col in data[vars].columns:\n",
    "            n_bin = math.ceil(1 + 3.32*math.log10(len(data)))\n",
    "            binwidth = ( data[col].max() - data[col].min() ) / n_bin\n",
    "            binwidths.append(binwidth)\n",
    "            del binwidth\n",
    "    \n",
    "    # 4) 설정한 binwidth를 조정하는 비율\n",
    "    if binwidth_adj_ratio is not None:\n",
    "        binwidths = [binwidth * binwidth_adj_ratio for binwidth in binwidths]\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    # 5) vars 별로 plot 생성\n",
    "    for iter,var in enumerate(vars):\n",
    "        \n",
    "        binwidth = binwidths[iter]\n",
    "        \n",
    "        # (1) histogram\n",
    "        ax1 = fig.add_subplot(nrow, ncol, iter+1)\n",
    "        g1 = sns.histplot(data = data, x = var, hue = hue,\n",
    "                          kde = True, stat = 'probability', \n",
    "                          color = 'lightskyblue',\n",
    "                          binwidth = binwidth, ax = ax1)\n",
    "        ax2 = ax1.twinx()\n",
    "        \n",
    "        # (2) density plot\n",
    "        g2 = sns.kdeplot(data = data, x = var, hue = hue,\n",
    "                         color = 'red', lw = 2, ax = ax2)\n",
    "        ax2.set_ylim(0, ax1.get_ylim()[1] / binwidth)                  # similir limits on the y-axis to align the plots\n",
    "        #ax2.yaxis.set_major_formatter(PercentFormatter(1 / binwidth))  # show axis such that 1/binwidth corresponds to 100%\n",
    "        ax2.grid(False)\n",
    "        \n",
    "        # (3) density plot y축 없애기\n",
    "        g2.set(yticklabels=[]) \n",
    "        g2.set(ylabel=None)\n",
    "        g2.tick_params(right=False)\n",
    "        \n",
    "        a,b = divmod(iter,ncol)\n",
    "        if b!=0:\n",
    "            g1.set(ylabel=None)\n",
    "        \n",
    "    # 안겹치도록 설정\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# example : density_plot(train, vars=num_vari)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.9. density plot : histogram + density plot\n",
    "#\n",
    "# (1) grp_var vs hue_var 막대그래프\n",
    "# (2) grp_var(x축), hue_var에 따른 각 num_var들의 barplot, violineplot, box+swarmplot + kdeplot\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "#- grp_var : x축 구분할 그룹변수 (text)\n",
    "#- num_vari : 숫자형 변수 (list)\n",
    "#- data : 기준 data\n",
    "#- title_text : plot title (text)\n",
    "#- hue_var : hue 그루핑변수\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def plot_num(grp_var, num_vari, df, title_text=None, hue_var=None):\n",
    "    \n",
    "    data = df.copy()\n",
    "    \n",
    "    # (1)번 그래프 setting\n",
    "    fig0 = plt.figure(figsize=(3,3))\n",
    "    ax0  = fig0.add_subplot(1,1,1)\n",
    "    \n",
    "    if title_text is None:\n",
    "        title_text = grp_var\n",
    "        \n",
    "    plt.title(title_text, loc='left', pad=20, fontdict={'fontsize' : 30,\n",
    "                                                        'fontweight' : 'bold',\n",
    "                                                        'color' : 'c'})\n",
    "    \n",
    "    # grp_var와 hue_var가 겹치는 경우, hue를 나누지 않음\n",
    "    if (grp_var!=hue_var) and (hue_var is not None):\n",
    "\n",
    "        ct = pd.crosstab(data[grp_var],data[hue_var])\n",
    "        ax = ct.plot(kind='bar', stacked=False, rot=0, ax=ax0)\n",
    "        ax.legend(title=hue_var, bbox_to_anchor=(1, 1.02), loc='upper left')\n",
    "        \n",
    "    else:\n",
    "        ct = data[grp_var].value_counts()\n",
    "        ax = ct.plot(kind='bar', stacked=False, rot=0, ax=ax0)\n",
    "        \n",
    "    # show\n",
    "    plt.xlabel('')\n",
    "    plt.show()\n",
    "    \n",
    "    # 숫자변수중에 [grp,id]변수가 있으면 제외\n",
    "    num_vari_x = list(set(num_vari) - set([grp_var,'id']))\n",
    "    \n",
    "    # plt 생성\n",
    "    fig = plt.figure(figsize=(15,15)) # figsize=(15,7)\n",
    "    plt.axis('off') # 안끄면 x축에 0~1까지 축생김\n",
    "    \n",
    "    for iter,var in enumerate(num_vari_x):\n",
    "\n",
    "        # hue랑 grp_var랑 같으면 hue를 넣지않음\n",
    "        hue_x = [None if grp_var==hue_var else hue_var][0]\n",
    "\n",
    "        # (n,4) plot\n",
    "        ax1 = fig.add_subplot(len(num_vari_x),4,4*iter+1)\n",
    "        ax2 = fig.add_subplot(len(num_vari_x),4,4*iter+2)\n",
    "        ax3 = fig.add_subplot(len(num_vari_x),4,4*iter+3)\n",
    "        ax4 = fig.add_subplot(len(num_vari_x),4,4*iter+4)\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "        # (2-1) 3번째 : box + swarm plot (ylim가져오기위해서 제일 먼저 실행)\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "        g11=sns.swarmplot(x=grp_var, y=var, data=data, ax = ax3, color='crimson', marker='*', s = 7)\n",
    "        g12=sns.boxplot  (x=grp_var, y=var, data=data, ax = ax3)\n",
    "        g12.set(ylabel=None)\n",
    "        g12.set(yticklabels=[])\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "        # (2-2) 1번째 : barplot\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "        ax1.set_ylim(ax3.get_ylim())\n",
    "        g21=sns.barplot(x=grp_var, y=var, data=data, ax=ax1, hue=hue_x)\n",
    "        # g21.set(ylabel=None)\n",
    "        # g21.set(yticklabels=[])\n",
    "        # g21.axes.set_title(str(iter+1) + ':' + var, fontsize=20, weight='bold', ha='left', x=-.05)\n",
    "        g21.set_ylabel(var,fontsize=20)\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "        # (2-3) 2번째 : violinplot\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "        ax2.set_ylim(ax3.get_ylim())\n",
    "        g31=sns.violinplot(x=grp_var, y=var, data=data, ax=ax2, legend=False, hue=hue_x)\n",
    "        g31.set(ylabel=None)\n",
    "        g31.set(yticklabels=[])\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "        # (2-4) 4번째 : density plot\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "        ax4.set_ylim(ax3.get_ylim())\n",
    "        g41=sns.kdeplot(y=var, hue=grp_var, data=data, ax=ax4)\n",
    "        g41.set(ylabel=None)\n",
    "        g41.set(yticklabels=[])\n",
    "        g41.tick_params(right=False)\n",
    "        g41.set(xlabel=None)\n",
    "        g41.set(xticklabels=[])\n",
    "        \n",
    "        # 맨 아래에만 x축이 생성되도록 setting\n",
    "        if (iter+1) != len(num_vari_x):\n",
    "            \n",
    "            g12.set(xlabel=None)\n",
    "            g12.set(xticklabels=[])\n",
    "\n",
    "            g21.set(xlabel=None)\n",
    "            g21.set(xticklabels=[])\n",
    "\n",
    "            g31.set(xlabel=None)\n",
    "            g31.set(xticklabels=[])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# # example\n",
    "# plot_num(grp_var = 'sex', num_vari = num_vari, hue_var = 'target',\n",
    "#          data = train, title_text = 'sex')\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.10. onehot encoding\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "#- data\n",
    "#- col_types\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def onehot_encoding(data, col_types):\n",
    "\n",
    "    raw_data = data.copy()\n",
    "    \n",
    "    cols = list(set(data.columns) - set(['target']))\n",
    "\n",
    "    for col in cols:\n",
    "        if col_types[col]==str:\n",
    "\n",
    "            data = pd.concat([\n",
    "                data.drop([col],axis=1).reset_index(drop=True),\n",
    "                pd.get_dummies(data[col], prefix = col).reset_index(drop=True).apply(lambda x:x.astype(float))\n",
    "                ],\n",
    "                axis=1)\n",
    "    \n",
    "    return(data)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.10. onehot encoding : str들 모두 int/category로 바꾸기\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "#- data\n",
    "#- col_types\n",
    "#- convert : int / category\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def str_convert(data, col_types, convert = [int,'category']):\n",
    "\n",
    "    cols = list(set(data.columns) - set(['target']))\n",
    "\n",
    "    for col in cols:\n",
    "        if col_types[col]==str:\n",
    "            data[col] = data[col].astype(convert)\n",
    "    \n",
    "    return(data)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.11. setdiff\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def setdiff(x,y):\n",
    "    return(list(set(x)-set(y)))\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.12. automl_comp\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# missing value를 auto ml로 예측하기위함\n",
    "# target이 missing인 값들을 제외하고, kfold cv를 통해서 각 알고리즘들의 scoring을 계산\n",
    "\n",
    "# # cross_val_score에서 쓸 수 있는 scoring\n",
    "# list(sklearn.metrics.SCORERS.keys())\n",
    "\n",
    "def AUTOML_COMP(train,\n",
    "                target,\n",
    "                col_types,\n",
    "                objective,\n",
    "                scoring = None,\n",
    "                test = None,\n",
    "                ignore_features = None,\n",
    "                n_splits = 10,\n",
    "                missing_return = True,\n",
    "                fit_model = False):\n",
    "\n",
    "    # copy\n",
    "    X_train = train.drop([target],axis=1)\n",
    "    y_train = train[target]\n",
    "    if test is not None:\n",
    "        X_test = test.drop([target],axis=1)\n",
    "        y_test = test[target]\n",
    "    \n",
    "    # remove needless features\n",
    "    features = list(set(X_train.columns)-set(ignore_features))\n",
    "\n",
    "    # 모형적합\n",
    "    start_time = time.time()\n",
    "\n",
    "    # binary / regression\n",
    "    models = []\n",
    "    if objective=='binary':\n",
    "        \n",
    "        if scoring is None:\n",
    "            scoring = 'accuracy'\n",
    "        \n",
    "        models.append(('LR',LogisticRegression(solver='liblinear',multi_class = 'ovr',random_state=SEED)))\n",
    "        models.append(('LDA',LinearDiscriminantAnalysis()))\n",
    "        models.append(('KNN',KNeighborsClassifier()))\n",
    "        models.append(('CART',DecisionTreeClassifier(random_state=SEED)))\n",
    "        models.append(('NB',GaussianNB())) # Gaussian Naive Bayes\n",
    "        models.append(('SVM',SVC(gamma='auto',random_state=SEED)))\n",
    "        models.append(('RFC',RandomForestClassifier(random_state=SEED)))\n",
    "        models.append(('XGBC',XGBClassifier(iterations=10000,verbosity = 0,random_state=SEED)))\n",
    "        models.append(('LGBMC',LGBMClassifier(random_state=SEED)))\n",
    "        models.append(('AdaC',AdaBoostClassifier(random_state=SEED)))\n",
    "        models.append(('Cat',CatBoostClassifier(iterations=10000,silent=True,random_state=SEED)))\n",
    "\n",
    "    elif objective=='regression':\n",
    "        \n",
    "        if scoring is None:\n",
    "            scoring = 'neg_mean_squared_error'\n",
    "\n",
    "        models.append(('LR',LinearRegression()))\n",
    "        models.append(('RIDGE',RidgeClassifier()))\n",
    "        models.append(('LASSO',Lasso(random_state=SEED)))\n",
    "        models.append(('KNN',KNeighborsRegressor()))\n",
    "        models.append(('CART',DecisionTreeRegressor(random_state=SEED)))\n",
    "        models.append(('EN',ElasticNet(random_state=SEED)))\n",
    "        models.append(('SVM',SVR()))\n",
    "        models.append(('RFR',RandomForestRegressor(random_state=SEED)))\n",
    "        models.append(('XGBR',XGBRegressor(iterations=10000,verbosity=0,random_state=SEED)))\n",
    "        models.append(('LGBMR',LGBMRegressor(random_state=SEED)))\n",
    "        models.append(('AdaR',AdaBoostRegressor(random_state=SEED)))\n",
    "        models.append(('Cat',CatBoostRegressor(iterations=10000,silent=True,random_state=SEED)))\n",
    "\n",
    "    # onehot encoding\n",
    "    X_train_onehot = onehot_encoding(X_train[features], col_types)\n",
    "    if test is not None:\n",
    "        X_test_onehot = onehot_encoding(X_test[features], col_types)\n",
    "        \n",
    "    # remove missing y value\n",
    "    X_tr = X_train_onehot[~y_train.isnull()]\n",
    "    y_tr = y_train       [~y_train.isnull()]\n",
    "    if test is not None:\n",
    "        X_te = X_test_onehot[~y_test.isnull()]\n",
    "\n",
    "    # kfold cross validation\n",
    "    if fit_model:\n",
    "        \n",
    "        results = []\n",
    "        names   = []\n",
    "        msgs    = []\n",
    "\n",
    "        pbar = tqdm(models)\n",
    "        for name, model in pbar:\n",
    "            pbar.set_description(f'fitting... ({name})')\n",
    "\n",
    "            kfold = KFold(n_splits=n_splits,random_state=SEED,shuffle = True)\n",
    "            cv_results = cross_val_score(model,\n",
    "                                         X = X_tr,\n",
    "                                         y = y_tr,\n",
    "                                         cv = kfold,\n",
    "                                         scoring = scoring,\n",
    "                                         verbose = 0)\n",
    "            results.append(cv_results)\n",
    "            names.append(name)\n",
    "            msgs.append((name,cv_results.mean(),cv_results.std()))\n",
    "\n",
    "        end_time = time.time()\n",
    "        running_time = (end_time - start_time)/60\n",
    "        running_time = f'{running_time:.1f} Mins'\n",
    "\n",
    "    ret = {}\n",
    "    \n",
    "    ret['X_train'] = X_tr\n",
    "    ret['y_train'] = y_tr\n",
    "    \n",
    "    if fit_model:\n",
    "        ret['run_time'] = running_time\n",
    "        ret['message'] = msgs\n",
    "        ret['model'] = models\n",
    "        ret['cv_result'] = results\n",
    "\n",
    "    if test is not None:\n",
    "        ret['X_test']  = X_te\n",
    "    \n",
    "    if missing_return:\n",
    "        ret['train_missing_return'] = X_train_onehot[y_train.isnull()]\n",
    "        if test is not None:\n",
    "            ret['test_missing_return'] = X_test_onehot[y_test.isnull()]\n",
    "    \n",
    "    return(ret)\n",
    "\n",
    "def AUTO_COMP_PLOT(object, title = 'Algorithm Comparision'):\n",
    "\n",
    "    cv_res_df = pd.DataFrame(\n",
    "        np.transpose(object['cv_result']), \n",
    "        columns = [name for name,model in object['model']]\n",
    "    )\n",
    "\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    fig.add_subplot(111)\n",
    "    p = sns.boxplot(data = cv_res_df)\n",
    "    p = sns.swarmplot(data = cv_res_df,marker='*', s = 7,color='crimson')\n",
    "    p.set_title(title,fontsize = 20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f20af",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32e1f61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:28.089968Z",
     "start_time": "2022-01-08T06:27:28.073966Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053f33a6",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "># **Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18474b8",
   "metadata": {},
   "source": [
    "데이터를 제공한 회사는 ABC Private Limited로 소매회사임. \\\n",
    "회사설명 : 작물 재배, 시장 원예, 원예(Growing of crops, market gardening, horticulture) [Link](https://www.zaubacorp.com/company/A-B-C-PRIVATE-LIMITED/U01110MH1950PTC008007)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402d6df4",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 변수정보 (변수명 참조 : [DataHackaton](https://datahack.analyticsvidhya.com/contest/black-friday/#ProblemStatement))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce28384f",
   "metadata": {},
   "source": [
    "|변수명 | 변수정보 |\n",
    "|:---:|:---|\n",
    "| User_ID | 사용자 ID |\n",
    "| Product_ID | 제품 ID |\n",
    "| Gender | 사용자의 성별 |\n",
    "| Age | 나이(구간) |\n",
    "| Occupation | 직업(마스킹됨) |\n",
    "| City_Category | 도시의 범주(A,B,C) |\n",
    "| Stay_In_Current_City_Years | 현재 도시에 체류한 기간 |\n",
    "| Marital_Status | 결혼 여부 |\n",
    "| Product_Category_1 | 제품 카테고리 (마스킹됨) |\n",
    "| Product_Category_2 | 제품 카테고리2 (마스킹됨,다른 카테고리에도 속할 수 있음) |\n",
    "| Product_Category_3 | 제품 카테고리3 (마스킹됨,다른 카테고리에도 속할 수 있음) |\n",
    "| Purchase | 구매금액(Target) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda586bc",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4c0dc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:29.231613Z",
     "start_time": "2022-01-08T06:27:28.092968Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "COL_TYPE = {\n",
    "    'User_ID'                     : str,\n",
    "    'Product_ID'                  : str,\n",
    "    'Gender'                      : str,\n",
    "    'Age'                         : str,\n",
    "    'Occupation'                  : str,\n",
    "    'City_Category'               : str,\n",
    "    'Stay_In_Current_City_Years'  : str,\n",
    "    'Marital_Status'              : str,\n",
    "    'Product_Category_1'          : str,\n",
    "    'Product_Category_2'          : str,\n",
    "    'Product_Category_3'          : str,\n",
    "    'Purchase'                    : float\n",
    "}\n",
    "\n",
    "# Train Data Load (550,068 rows, 12 columns)\n",
    "train = pd.read_csv(DATA_PATH + 'train.csv', dtype = COL_TYPE)\n",
    "test  = pd.read_csv(DATA_PATH + 'test.csv' , dtype = COL_TYPE)\n",
    "sub   = pd.read_csv(DATA_PATH + 'sample_submission.csv', dtype = COL_TYPE)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3144513a",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Column 명 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a655303",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:29.298122Z",
     "start_time": "2022-01-08T06:27:29.234613Z"
    }
   },
   "outputs": [],
   "source": [
    "COL_NAME = {\n",
    "    'User_ID'                     : 'user_id',\n",
    "    'Product_ID'                  : 'prod_id',\n",
    "    'Gender'                      : 'gender',\n",
    "    'Age'                         : 'age',\n",
    "    'Occupation'                  : 'occupation',\n",
    "    'City_Category'               : 'city_cat',\n",
    "    'Stay_In_Current_City_Years'  : 'stay_year',\n",
    "    'Marital_Status'              : 'marital',\n",
    "    'Product_Category_1'          : 'prod_cat_1',\n",
    "    'Product_Category_2'          : 'prod_cat_2',\n",
    "    'Product_Category_3'          : 'prod_cat_3',\n",
    "    'Purchase'                    : 'target',\n",
    "}\n",
    "\n",
    "def rename_fn(df,name_dict,type_dict):\n",
    "\n",
    "    data = df.copy()\n",
    "    \n",
    "    for key in name_dict.keys():\n",
    "        if key in data.columns:\n",
    "            data.rename(columns={f'{key}': name_dict[key]}, inplace = True)\n",
    "        \n",
    "    return(data)\n",
    "\n",
    "train2 = rename_fn(train, COL_NAME, COL_TYPE)\n",
    "test2  = rename_fn(test , COL_NAME, COL_TYPE)\n",
    "sub2   = rename_fn(sub  , COL_NAME, COL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5dc3af",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### COL_TYPE 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b26333",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:29.312623Z",
     "start_time": "2022-01-08T06:27:29.299122Z"
    }
   },
   "outputs": [],
   "source": [
    "for (old_key,old_value),(new_key,new_value) in zip(sorted(COL_TYPE.items()),sorted(COL_NAME.items())):\n",
    "    COL_TYPE[new_value] = COL_TYPE.pop(old_key)\n",
    "\n",
    "COL_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c4ebf0",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Missing Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e0e065",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:41.577181Z",
     "start_time": "2022-01-08T06:27:29.314124Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(color.BOLD + color.BLUE + '> # of train Missing : \\n' + color.END, missing_column_check(train2, COL_TYPE), '\\n')\n",
    "print(color.BOLD + color.BLUE + '> # of test  Missing : \\n' + color.END, missing_column_check(test2 , COL_TYPE), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8208a496",
   "metadata": {},
   "source": [
    "Product_Category_2, 3에서 많은 Null이 있음 \\\n",
    "정확한 Missing value 분석은 EDA에서 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995d1761",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "># **EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e31a19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:41.636188Z",
     "start_time": "2022-01-08T06:27:41.578681Z"
    }
   },
   "outputs": [],
   "source": [
    "# 숫자형변수, 문자형변수\n",
    "num_vari  = [key for key in COL_TYPE.keys() if COL_TYPE[key] in [int,float]]\n",
    "char_vari = [key for key in COL_TYPE.keys() if COL_TYPE[key] in [str      ]]\n",
    "\n",
    "num_df  = train2[num_vari ]\n",
    "char_df = train2[char_vari]\n",
    "\n",
    "print('전체 변수 :', len(train.columns))\n",
    "print('숫자 변수 :', len(num_vari),  ':', num_vari)\n",
    "print('문자 변수 :', len(char_vari), ':', char_vari)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc21bfe7",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Characteristic Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed60241",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:42.256767Z",
     "start_time": "2022-01-08T06:27:41.637188Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_unique = 5\n",
    "\n",
    "plot_char_vari = sorted(list(set(char_vari)-set(['id'])))\n",
    "\n",
    "max_char_len = max([len(var) for var in plot_char_vari])\n",
    "for xvar in plot_char_vari:\n",
    "    blank_var = ' '*(max_char_len-len(xvar))\n",
    "    \n",
    "    unique_val = sorted(train2[xvar].dropna().unique())\n",
    "    if len(unique_val)>=max_unique:\n",
    "        example = unique_val[:max_unique] + ['...']\n",
    "    else:\n",
    "        example = unique_val\n",
    "    \n",
    "    print(f'{xvar} {blank_var}: {example}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14505e5c",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### 문자형 변수 짧게 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a9b48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:42.306273Z",
     "start_time": "2022-01-08T06:27:42.258267Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing(_df):\n",
    "    df = _df.copy()\n",
    "    \n",
    "    #------------------------------------------------------------#\n",
    "    # 1. species, island, sex, clutch_completion : 첫글자\n",
    "    #------------------------------------------------------------#\n",
    "    \n",
    "    return df\n",
    "\n",
    "train3 = preprocessing(train2)\n",
    "test3  = preprocessing(test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe607aa0",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Plot : 1-Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b688555",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:53.423685Z",
     "start_time": "2022-01-08T06:27:42.307774Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_char_vari = setdiff(plot_char_vari,['user_id','prod_id'])\n",
    "\n",
    "fig = plt.figure(figsize=(15*1.2,7*1.5))\n",
    "nrow = np.ceil(np.sqrt(len(plot_char_vari)))\n",
    "\n",
    "iter=0\n",
    "for var in sorted(plot_char_vari):\n",
    "    iter+=1\n",
    "    \n",
    "    # ordering\n",
    "    unique_x = train3[var].dropna().unique()\n",
    "    try:\n",
    "        order_x = [str(x) for x in sorted(unique_x.astype(int))]\n",
    "    except ValueError:\n",
    "        order_x = sorted(unique_x)\n",
    "        \n",
    "    fig.add_subplot(nrow,nrow,iter)\n",
    "    ax = sns.countplot(train3[var],\n",
    "                       order = order_x)\n",
    "    ax.set_xlabel(var, size=15)\n",
    "    ax.set_ylabel('Count',size=15)\n",
    "\n",
    "    for p in ax.patches:\n",
    "        count_x = p.get_height()\n",
    "        sum_x   = sum([p.get_height() for p in ax.patches])\n",
    "        ratio_x = ( p.get_height() / sum_x )*100\n",
    "        ann_x   = f'{count_x:,}\\n({ratio_x:.1f}%)'\n",
    "        ax.annotate(ann_x, (p.get_x() + 0.0, p.get_height() +50), size = 12, color='white')\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993cb960",
   "metadata": {},
   "source": [
    "|변수명 | 정보 |\n",
    "|:---:|:---|\n",
    "| age | 미성년자의 비중이 적음, 26-35세의 비중이 제일 큼, 18-45세의 비중이 78%로 매우 높음 |\n",
    "| city_cat | B의 비중이 제일 높음. B>C>A의 순서 |\n",
    "| gender | 남자고객의 비중이 75%로 매우 높음 |\n",
    "| marital | 기혼이 41%, 미혼이 59% |\n",
    "| occupation | 빈도가 높은 0,4,7 등의 경우는 무직,학생,주부,회사원과 같은 일반적인 카테고리일 가능성이 높음 <br> 이런 카테고리는 일반 구매자일 가능성이 높고, count가 낮은 카테고리들은 법인일 가능성이 있음 <br> -> 카테고리 별 구매금액 확인 및 clustering을 해보는것도 좋을 것 같음 |\n",
    "| prod_cat_1 | 1,5,8번의 경우에는 잘팔리는 것들(major), 나머지는 minor한 것들 <br> prod_cat_2,3에 비해서 분명하게 나뉘는 것들이 많음 |\n",
    "| stay_year | 현재도시에 체류한 기간이 1년인게 1/3로 가장 많고, 나머지는 비슷한 수준 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6933807",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Plot : 2-Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6db50d",
   "metadata": {},
   "source": [
    "문자열 조합의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462ec6bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:53.449188Z",
     "start_time": "2022-01-08T06:27:53.444688Z"
    }
   },
   "outputs": [],
   "source": [
    "n_comb = math.comb(len(plot_char_vari),2)\n",
    "ncol = math.ceil(np.sqrt(n_comb))\n",
    "nrow = math.ceil(n_comb/ncol)\n",
    "\n",
    "print(f'n_comb : {n_comb}, (nrow,ncol) : ({nrow},{ncol})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e66d433",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "그룹이 많은 prod_cat은 빼고 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66bf12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:56.561083Z",
     "start_time": "2022-01-08T06:27:53.454689Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_var = setdiff(plot_char_vari,['prod_cat_1','prod_cat_2','prod_cat_3'])\n",
    "n_comb = math.comb(len(check_var),2)\n",
    "\n",
    "ncol = math.ceil(np.sqrt(n_comb))\n",
    "nrow = math.ceil(n_comb/ncol)\n",
    "\n",
    "comb = list(itertools.product([True, False], repeat=len(check_var)))\n",
    "comb = [x for x in comb if sum(x)==2]\n",
    "\n",
    "for iter,c in enumerate(comb):\n",
    "    xvar,yvar = np.array(check_var)[[c]]\n",
    "    print(color.BOLD+color.BLUE+f'> ({iter+1}/{len(comb)}) ({xvar},{yvar})'+color.END)\n",
    "    \n",
    "    View(pd.crosstab(train3[xvar].fillna('nan'),\n",
    "                     train3[yvar].fillna('nan')))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be60a43e",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "prod_cat_2,3의 missing을 채우기 위해서 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea2e679",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:58.724858Z",
     "start_time": "2022-01-08T06:27:56.564584Z"
    }
   },
   "outputs": [],
   "source": [
    "except_prod_1 = setdiff(plot_char_vari,['prod_cat_1'])\n",
    "\n",
    "for iter,var in enumerate(except_prod_1):\n",
    "    xvar = var\n",
    "    yvar = 'prod_cat_1'\n",
    "    print(color.BOLD+color.BLUE+f'> ({iter+1}/{len(except_prod_1)}) ({xvar})'+color.END)\n",
    "    \n",
    "    View(pd.crosstab(train3[xvar].fillna('nan'),\n",
    "                     train3[yvar].fillna('nan')))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5728c7e",
   "metadata": {},
   "source": [
    "별다른 큰 패턴이 보이지 않음.. → ML로 결측값을 채워넣거나, 삭제\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc055e9",
   "metadata": {},
   "source": [
    "모든 문자형변수 조합의 2D plot <br>\n",
    "각 변수의 그룹의 개수가 많아서 지저분해보임 → 주석처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a439a5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:58.729859Z",
     "start_time": "2022-01-08T06:27:58.726358Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# if PLOT:\n",
    "\n",
    "#     check_var = setdiff(plot_char_vari,['prod_cat_1','prod_cat_2','prod_cat_3'])\n",
    "    \n",
    "#     n_comb = math.comb(len(check_var),2)\n",
    "\n",
    "#     ncol = math.ceil(np.sqrt(n_comb))\n",
    "#     nrow = math.ceil(n_comb/ncol)\n",
    "\n",
    "#     comb = list(itertools.product([True, False], repeat=len(check_var)))\n",
    "#     comb = [x for x in comb if sum(x)==2]\n",
    "\n",
    "#     fig  = plt.figure(figsize=(15*3,7*4))\n",
    "\n",
    "#     tick_size      = 40\n",
    "#     label_size     = 40\n",
    "#     freq_font_size = 30\n",
    "#     title_size     = 50\n",
    "#     legend_size    = 30\n",
    "\n",
    "#     p1_iter = 0\n",
    "#     change_row = 0\n",
    "#     for iter,c in enumerate(comb):\n",
    "\n",
    "#         if change_row==(ncol):\n",
    "#             change_row  = 1 # go back\n",
    "#             p1_iter    += ncol+1\n",
    "#         else:\n",
    "#             change_row += 1\n",
    "#             p1_iter    += 1\n",
    "\n",
    "#         p2_iter = p1_iter+ncol\n",
    "\n",
    "#         # # check\n",
    "#         # print(f'change_row : {change_row}, p1_iter : {p1_iter}, p2_iter : {p2_iter}')\n",
    "\n",
    "#         xvar,yvar = np.array(check_var)[[c]]\n",
    "\n",
    "#         # (1) crosstab plot\n",
    "#         fig.add_subplot(2*nrow, ncol, p1_iter)\n",
    "#         ct = pd.crosstab(train3[xvar],train3[yvar]).sort_index(level=0, ascending=True).sort_index(level=1, ascending=True)\n",
    "#         p0 = sns.heatmap(ct.T, annot=True, fmt='.0f', cbar=False, annot_kws={\"size\": freq_font_size}, cmap='YlGnBu')\n",
    "#         p0.tick_params(axis = 'y', labelsize=tick_size)\n",
    "#         p0.tick_params(axis = 'x', labelsize=0)\n",
    "#         p0.set_title('-'*27 + f'{iter+1}' + '-'*27,fontsize=title_size)\n",
    "#         p0.set_xlabel('')\n",
    "#         p0.set_ylabel(f'{yvar}',fontsize=label_size)\n",
    "\n",
    "#         # (2) count plot\n",
    "#         fig.add_subplot(2*nrow, ncol, p2_iter)\n",
    "#         p1 = sns.countplot(train3[xvar], hue = train3[yvar], dodge=True, palette = 'Set1',\n",
    "#                            order     = sorted(train3[xvar].value_counts().index),\n",
    "#                            hue_order = sorted(train3[yvar].value_counts().index))\n",
    "#         p1.tick_params(labelsize=tick_size)\n",
    "#         p1.set_xlabel(f'{xvar}',fontsize=label_size)\n",
    "#         p1.set_ylabel('Count',fontsize=label_size)\n",
    "#         p1.get_legend().remove()\n",
    "#         #plt.legend(bbox_to_anchor=(1.02, 1), \n",
    "#         #           loc=2, borderaxespad=0, fontsize=legend_size)\n",
    "\n",
    "# #         # show freq. into plot\n",
    "# #         values=train3[xvar].value_counts().values\n",
    "# #         for j, g1 in enumerate(p1.patches):\n",
    "# #             p1.annotate(f'\\n{g1.get_height()}', (g1.get_x()+0.13, g1.get_height()+1.1), ha='center', va='top', color='white', size=freq_font_size)\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3d449a",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Numeric Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4101543a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:58.779365Z",
     "start_time": "2022-01-08T06:27:58.731359Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "round(train3.describe(),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d64b4d",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Plot : 1-Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb6124b",
   "metadata": {},
   "source": [
    "#### Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b1115",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:27:58.980391Z",
     "start_time": "2022-01-08T06:27:58.780865Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15*0.5,7*0.5))\n",
    "\n",
    "# len(num_vari) : 6\n",
    "\n",
    "nrow = 1\n",
    "ncol = 1\n",
    "\n",
    "if PLOT:\n",
    "    \n",
    "    for iter,var in enumerate(num_vari,1):\n",
    "        fig.add_subplot(nrow,ncol,iter)\n",
    "        sns.boxplot(y=train3[var])\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5764bd50",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "#### hist + kde plot (hue=target)\n",
    "<br></br>\n",
    "각 숫자형변수별 분포 확인 & target에 따른 분포확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d559f68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:28:04.867638Z",
     "start_time": "2022-01-08T06:27:58.982891Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "\n",
    "    # No hue\n",
    "    print(color.BOLD + color.BLUE + '> No Group' + color.END)\n",
    "    density_plot(train3,\n",
    "                 vars = sorted(set(num_vari) - set(['id'])),\n",
    "                 binwidth_adj_ratio = 0.8,\n",
    "                 figsize = (15*0.5,7*0.5))\n",
    "    plt.show()\n",
    "\n",
    "#     # hue : target\n",
    "#     print(color.BOLD + color.BLUE + '> Group by Target' + color.END)\n",
    "#     density_plot(train,\n",
    "#                  vars = set(num_vari) - set(['id']),\n",
    "#                  hue = 'target',\n",
    "#                  binwidth_adj_ratio = 0.8)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70f9eae",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Plot : 2-Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d4424",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "#### Pairplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b73b39",
   "metadata": {},
   "source": [
    "숫자형 변수가 target 하나라서 주석처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51584c01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:28:04.871639Z",
     "start_time": "2022-01-08T06:28:04.869138Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# if PLOT:\n",
    "\n",
    "#     pairplot_df = num_df.copy()\n",
    "\n",
    "#     sns.pairplot(pairplot_df, corner=True)#, hue = 'target')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba16495",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "#### Clustermap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0fb979",
   "metadata": {},
   "source": [
    "숫자형 변수가 target 하나라서 주석처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ac1c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T06:28:04.883140Z",
     "start_time": "2022-01-08T06:28:04.873139Z"
    }
   },
   "outputs": [],
   "source": [
    "# if PLOT:\n",
    "\n",
    "#     pairplot_df = num_df.copy()\n",
    "    \n",
    "#     sns.clustermap(pairplot_df.corr(), annot=True, cmap = 'RdYlBu_r')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed0b992",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Numeric Variable * Characteristic Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47a0edb",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ea829f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-07T22:33:54.852019Z",
     "start_time": "2022-01-07T22:19:24.826257Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "\n",
    "    for _iter,_col in enumerate(sorted(plot_char_vari)):\n",
    "        plot_num(grp_var = _col, num_vari = num_vari, #hue_var='target',\n",
    "                 df = train3,\n",
    "                 title_text = str(_iter+1) + '. ' + _col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea7b37b",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24598a84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.364079Z",
     "start_time": "2022-01-06T18:55:50.353578Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "\n",
    "    for _iter,_col in enumerate(sorted(plot_char_vari)):\n",
    "        plot_num(grp_var = _col, num_vari = list(set(num_vari)-set(['body_mass'])), #hue_var='target',\n",
    "                 df = test3,\n",
    "                 title_text = str(_iter+1) + '. ' + _col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f0c1a8",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "># **Missing Prediction**\n",
    "\n",
    "EDA에서 (sex, delta_13c, delta_15n)에 대해서 별다른 분류를 할 수 없었음 <br>\n",
    "→ 모형적합으로 Missing 값을 예측 <br>\n",
    "\n",
    "- sex 예측은 분류 <br>\n",
    "- delta_13c, delta_15n 예측은 회귀 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0c796e",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 'nan' or 'null' → np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c059ca19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.406084Z",
     "start_time": "2022-01-06T18:55:50.365579Z"
    }
   },
   "outputs": [],
   "source": [
    "def str_nan_convert(df,col_types):\n",
    "    \n",
    "    _df = df.copy()\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col_types[col]==str:\n",
    "            df[col][df[col]=='nan']  = np.nan\n",
    "            df[col][df[col]=='null'] = np.nan\n",
    "            \n",
    "    return(df)\n",
    "\n",
    "train4 = str_nan_convert(train3,COL_TYPE)\n",
    "test4  = str_nan_convert(test3 ,COL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48825974",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331b24a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.422586Z",
     "start_time": "2022-01-06T18:55:50.407084Z"
    }
   },
   "outputs": [],
   "source": [
    "train4[train4['sex'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4efe37",
   "metadata": {},
   "source": [
    "delta_13c, delta_15n은 missing이 있으므로 제외\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5218a1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.450590Z",
     "start_time": "2022-01-06T18:55:50.424087Z"
    }
   },
   "outputs": [],
   "source": [
    "automl_comp_sex = AUTOML_COMP(train = train4,\n",
    "                              test = test4,\n",
    "                              target = 'sex',\n",
    "                              col_types = COL_TYPE,\n",
    "                              objective = 'binary',\n",
    "                              ignore_features = ['id','body_mass','delta_13c','delta_15n'],\n",
    "                              missing_return = True,\n",
    "                              fit_model = [True if MISSING_CHECK else False][0])\n",
    "\n",
    "if MISSING_CHECK:\n",
    "    AUTO_COMP_PLOT(automl_comp_sex, title = f'Algorithm Comparision (sex)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c038d213",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.455591Z",
     "start_time": "2022-01-06T18:55:50.452090Z"
    }
   },
   "outputs": [],
   "source": [
    "if MISSING_CHECK:\n",
    "    pd.DataFrame(automl_comp_sex['message'], columns = ['name','mean','std']).round(2).sort_values(['mean'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb3b73",
   "metadata": {},
   "source": [
    "ML알고리즘 비교했을 때, Mean/Median Accuracy가 높은 LDA로 sex를 예측\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4563542c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.479594Z",
     "start_time": "2022-01-06T18:55:50.457591Z"
    }
   },
   "outputs": [],
   "source": [
    "train5 = train4.copy()\n",
    "test5  = test4 .copy()\n",
    "\n",
    "sex_model = LinearDiscriminantAnalysis()\n",
    "sex_model.fit(X = automl_comp_sex['X_train'],\n",
    "              y = automl_comp_sex['y_train'])\n",
    "\n",
    "train5['sex'][train5['sex'].isnull()] = pd.Series(sex_model.predict(automl_comp_sex['train_missing_return'])).values\n",
    "test5 ['sex'][test5 ['sex'].isnull()] = pd.Series(sex_model.predict(automl_comp_sex['test_missing_return' ])).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cd7477",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.487595Z",
     "start_time": "2022-01-06T18:55:50.481594Z"
    }
   },
   "outputs": [],
   "source": [
    "print(cnt(train5['sex'][train4['sex'].isnull()]), cnt(test5 ['sex'][test4 ['sex'].isnull()]))\n",
    "print(cnt(train5['sex']), cnt(test5['sex']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ede7b",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## delta_13c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2257d7bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.512598Z",
     "start_time": "2022-01-06T18:55:50.489595Z"
    }
   },
   "outputs": [],
   "source": [
    "train5[train5['delta_13c'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8def58",
   "metadata": {},
   "source": [
    "delta_15n은 missing이 있으므로 제외, sex는 위에서 예측했으므로 포함\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba4f49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.555103Z",
     "start_time": "2022-01-06T18:55:50.514098Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "automl_comp_d13 = AUTOML_COMP(train = train5,\n",
    "                              test = test5,\n",
    "                              target = 'delta_13c',\n",
    "                              col_types = COL_TYPE,\n",
    "                              objective = 'regression',\n",
    "                              ignore_features = ['id','body_mass','delta_15n'],\n",
    "                              missing_return = True,\n",
    "                              fit_model = [True if MISSING_CHECK else False][0])\n",
    "\n",
    "if MISSING_CHECK:\n",
    "    AUTO_COMP_PLOT(automl_comp_d13, title = f'Algorithm Comparision (delta_13c)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d1369",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.561104Z",
     "start_time": "2022-01-06T18:55:50.556603Z"
    }
   },
   "outputs": [],
   "source": [
    "if MISSING_CHECK:\n",
    "    pd.DataFrame(automl_comp_d13['message'], columns = ['name','mean','std']).round(3).sort_values(['mean'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9887bec",
   "metadata": {},
   "source": [
    "ML알고리즘 비교했을 때, 안정적이고 Mean/Median negative MSE가 높은 Linear Regression으로 delta_13c를 예측\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b81ec0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.579606Z",
     "start_time": "2022-01-06T18:55:50.562604Z"
    }
   },
   "outputs": [],
   "source": [
    "train6 = train5.copy()\n",
    "test6  = test5 .copy()\n",
    "\n",
    "d13_model = LinearRegression()\n",
    "d13_model.fit(X = automl_comp_d13['X_train'],\n",
    "              y = automl_comp_d13['y_train'])\n",
    "\n",
    "train6['delta_13c'][train6['delta_13c'].isnull()] = pd.Series(d13_model.predict(automl_comp_d13['train_missing_return'])).values\n",
    "test6 ['delta_13c'][test6 ['delta_13c'].isnull()] = pd.Series(d13_model.predict(automl_comp_d13['test_missing_return' ])).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055deda4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.591608Z",
     "start_time": "2022-01-06T18:55:50.581106Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train6['delta_13c'][train5['delta_13c'].isnull()].values)\n",
    "print(test6 ['delta_13c'][test5 ['delta_13c'].isnull()].values)\n",
    "print(minmax(train6['delta_13c']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d954311",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## delta_15n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a43ef5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.611110Z",
     "start_time": "2022-01-06T18:55:50.593608Z"
    }
   },
   "outputs": [],
   "source": [
    "train6[train6['delta_15n'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33170613",
   "metadata": {},
   "source": [
    "sex, delta_13c는 위에서 예측했으므로 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6221d16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.644615Z",
     "start_time": "2022-01-06T18:55:50.612610Z"
    }
   },
   "outputs": [],
   "source": [
    "automl_comp_d15 = AUTOML_COMP(train = train6,\n",
    "                              test = test6,\n",
    "                              target = 'delta_15n',\n",
    "                              col_types = COL_TYPE,\n",
    "                              objective = 'regression',\n",
    "                              ignore_features = ['id','body_mass'],\n",
    "                              missing_return = True,\n",
    "                              fit_model = [True if MISSING_CHECK else False][0])\n",
    "\n",
    "if MISSING_CHECK:\n",
    "    AUTO_COMP_PLOT(automl_comp_d15, title = f'Algorithm Comparision (delta_15n)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3913cb82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.652616Z",
     "start_time": "2022-01-06T18:55:50.649115Z"
    }
   },
   "outputs": [],
   "source": [
    "if MISSING_CHECK:\n",
    "    pd.DataFrame(automl_comp_d15['message'], columns = ['name','mean','std']).round(3).sort_values(['mean'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6a7fc",
   "metadata": {},
   "source": [
    "ML알고리즘 비교했을 때, 안정적이고 Mean/Median negative MSE가 높은 AdaBoost로 delta_15n를 예측\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cca791d",
   "metadata": {},
   "source": [
    "mean / sd가 모두 0인 LR으로 delta_15n를 예측\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ddc9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.755629Z",
     "start_time": "2022-01-06T18:55:50.654116Z"
    }
   },
   "outputs": [],
   "source": [
    "train7 = train6.copy()\n",
    "test7  = test6 .copy()\n",
    "\n",
    "d15_model = AdaBoostRegressor()\n",
    "d15_model.fit(X = automl_comp_d15['X_train'],\n",
    "              y = automl_comp_d15['y_train'])\n",
    "\n",
    "train7['delta_15n'][train7['delta_15n'].isnull()] = pd.Series(d15_model.predict(automl_comp_d15['train_missing_return'])).values\n",
    "test7 ['delta_15n'][test7 ['delta_15n'].isnull()] = pd.Series(d15_model.predict(automl_comp_d15['test_missing_return' ])).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8518db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.762129Z",
     "start_time": "2022-01-06T18:55:50.757129Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train7['delta_15n'][train6['delta_15n'].isnull()].values)\n",
    "print(test7 ['delta_15n'][test6 ['delta_15n'].isnull()].values)\n",
    "print(minmax(train6['delta_15n']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6f9f21",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "># **Cluster**\n",
    "\n",
    "4.2.2.1에서 확인한 변수들끼리 kmeans로 Group 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5338d38a",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## culmen_dep & flipper_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb3651",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.795634Z",
     "start_time": "2022-01-06T18:55:50.763630Z"
    }
   },
   "outputs": [],
   "source": [
    "train8 = train7.copy()\n",
    "test8  = test7 .copy()\n",
    "\n",
    "# k-means\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(train8[['culmen_dep','flipper_len']])\n",
    "\n",
    "train8['cluster1'] = kmeans.labels_\n",
    "test8 ['cluster1'] = kmeans.predict(test8[['culmen_dep','flipper_len']])\n",
    "\n",
    "if PLOT:\n",
    "\n",
    "    sns.scatterplot(train8['culmen_dep'],train8['body_mass'], hue=train8['cluster1'],\n",
    "                    palette=['red','blue'])\n",
    "    plt.show()\n",
    "\n",
    "    # # 수기로 맞춰보기\n",
    "    # sns.scatterplot(train3['culmen_dep'],train3['flipper_len']**(1.565)+300 , color='blue')\n",
    "    # sns.scatterplot(train3['culmen_dep'],train3['body_mass']                , color='red')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c26273",
   "metadata": {},
   "source": [
    "4개정도 분류안됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa730a4",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## filpper_len & delta_15n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aa38f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.825638Z",
     "start_time": "2022-01-06T18:55:50.799634Z"
    }
   },
   "outputs": [],
   "source": [
    "# k-means\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(train8[['flipper_len','delta_15n']])\n",
    "\n",
    "train8['cluster2'] = kmeans.labels_\n",
    "test8 ['cluster2'] = kmeans.predict(test8[['flipper_len','delta_15n']])\n",
    "\n",
    "if PLOT:\n",
    "\n",
    "    sns.scatterplot(train8['flipper_len'], train8['body_mass'], hue=train8['cluster2'],\n",
    "                    palette=['red','blue'])\n",
    "    plt.show()\n",
    "\n",
    "    # # 수기로 맞춰보기\n",
    "    # sns.scatterplot(train3['flipper_len'],-(train3['delta_15n'])**(3.85)+8000 , color='blue')\n",
    "    # sns.scatterplot(train3['flipper_len'],  train3['body_mass']               , color='red')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6ff6a9",
   "metadata": {},
   "source": [
    "중간에 빨간색 하나 분류안됨\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b2fa42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.860642Z",
     "start_time": "2022-01-06T18:55:50.827138Z"
    }
   },
   "outputs": [],
   "source": [
    "print(color.BOLD+color.BLUE+'> train'+color.END)\n",
    "print(pd.crosstab(train8['cluster1'],train8['cluster2']))\n",
    "print(' ')\n",
    "print(color.BOLD+color.BLUE+'> test '+color.END)\n",
    "print(pd.crosstab(test8['cluster1'],test8['cluster2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63fc99e",
   "metadata": {},
   "source": [
    "train, test 모두 cluster1과 cluster2의 cross table 확인 : 모두 동일\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2347f06",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## final cluster variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453454ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.869143Z",
     "start_time": "2022-01-06T18:55:50.862642Z"
    }
   },
   "outputs": [],
   "source": [
    "train8['cluster'] = train8['cluster1']\n",
    "test8 ['cluster'] = test8 ['cluster1']\n",
    "\n",
    "del train8['cluster1'], train8['cluster2']\n",
    "del test8 ['cluster1'], test8 ['cluster2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac2c3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.880645Z",
     "start_time": "2022-01-06T18:55:50.870643Z"
    }
   },
   "outputs": [],
   "source": [
    "COL_TYPE['cluster'] = str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e6a91",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## EDA for final cluster variable\n",
    "<br></br>\n",
    "만든 Cluster를 기준으로 추가 EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202ffde4",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### cluster * Characteristic Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a91207",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.893146Z",
     "start_time": "2022-01-06T18:55:50.882145Z"
    }
   },
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "\n",
    "    plot_char_vari = set([col for col in COL_TYPE.keys() if COL_TYPE[col]==str]) - set(['id','cluster'])\n",
    "\n",
    "    fig  = plt.figure(figsize=(13,4))\n",
    "    for iter,var in enumerate(plot_char_vari):\n",
    "\n",
    "        # (1) cross table\n",
    "        fig.add_subplot(1,4,iter+1)\n",
    "        ct = pd.crosstab(train8['cluster'],train3[var])\n",
    "        p0 = sns.heatmap(ct.T, annot=True, fmt='.0f', cbar=False, annot_kws={\"size\": 20}, cmap='YlGnBu')\n",
    "        p0.tick_params(labelsize=20)\n",
    "        p0.set_title(f'{hue_var}',fontsize=30)\n",
    "        p0.set_xlabel('')\n",
    "        p0.set_ylabel('')\n",
    "        p0.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig  = plt.figure(figsize=(13,4))\n",
    "    for iter,var in enumerate(plot_char_vari):\n",
    "\n",
    "        # (2) count plot\n",
    "        fig.add_subplot(1,4,iter+1)\n",
    "        p1 = sns.countplot(train8[var], hue = train8['cluster'], dodge=True, palette = 'Set1',\n",
    "                           order     = sorted(train8[var]      .value_counts().index),\n",
    "                           hue_order = sorted(train8['cluster'].value_counts().index))\n",
    "        p1.tick_params(labelsize=15)\n",
    "        p1.set_xlabel(f'{var}',fontsize=15)\n",
    "        p1.set_ylabel('Count',fontsize=15)\n",
    "        plt.legend(bbox_to_anchor=(1.02, 1), \n",
    "                   loc=2, borderaxespad=0, fontsize=15)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102b2646",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### cluster * Numeric Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441e4ea4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.906148Z",
     "start_time": "2022-01-06T18:55:50.895646Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "    \n",
    "    sns.pairplot(train8, hue='cluster', corner=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6342f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.921650Z",
     "start_time": "2022-01-06T18:55:50.907648Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "\n",
    "    plot_num(grp_var = 'cluster', num_vari = num_vari, hue_var=None,\n",
    "             df = train8,\n",
    "             title_text = str(1) + '. ' + 'cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7dd3ee",
   "metadata": {},
   "source": [
    "대부분 cluster를 구분선으로 나뉘는 것으로 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3830410d",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "여기까지 대략 7분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3375f23a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.933651Z",
     "start_time": "2022-01-06T18:55:50.923150Z"
    }
   },
   "outputs": [],
   "source": [
    "f'{(time.time()-query_start_time)/60:.1f} Mins'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df41380",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "># **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3a24cf",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 건수 적은 변수들 합치기 & 파생변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a655351e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.950653Z",
     "start_time": "2022-01-06T18:55:50.935151Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing2(_df):\n",
    "    df = _df.copy()\n",
    "    \n",
    "    #------------------------------------------------------------#\n",
    "    # \n",
    "    #------------------------------------------------------------#\n",
    "    \n",
    "    return df\n",
    "\n",
    "train9 = preprocessing2(train8.copy())\n",
    "test9  = preprocessing2(test8 .copy())\n",
    "\n",
    "# # col type에 추가\n",
    "# for str_var in []:\n",
    "#     COL_TYPE[str_var] = str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a56cf2",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### 추가한 변수에 대해서 EDA 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26205f6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.962655Z",
     "start_time": "2022-01-06T18:55:50.953154Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for _iter,_col in enumerate(sorted([])):\n",
    "#     plot_num(grp_var = _col, num_vari = set(num_vari) - set('body_mass'), hue_var=None,\n",
    "#              data = train4,\n",
    "#              title_text = str(_iter+1) + '. ' + _col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b878b",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033d512",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.984658Z",
     "start_time": "2022-01-06T18:55:50.967656Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train10 = str_convert(train9, col_types = COL_TYPE, convert = 'category')\n",
    "test10  = str_convert(test9 , col_types = COL_TYPE, convert = 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1440262e",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "># **Segment** : segment를 구분하여 따로 모델 적합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df7c88",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 각 조합별 건수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2c5aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:50.989658Z",
     "start_time": "2022-01-06T18:55:50.986658Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # island, sex, species\n",
    "\n",
    "# segment0 = pd.Series(['1' if x=='B' else '0' for x in train3['island']])\n",
    "# segment1 = train3['sex']\n",
    "# segment2 = train3['species']\n",
    "\n",
    "# def comb_seg(n_comb, head):\n",
    "\n",
    "#     # 각 조합에 대해 건수를 추출\n",
    "#     res = []\n",
    "#     combination = list(itertools.product([1, 0], repeat=3))\n",
    "#     for comb in combination:\n",
    "        \n",
    "#         # n_comb와 맞는 것들만 추출\n",
    "#         if sum(comb)==n_comb:\n",
    "#             comb_number = np.where(np.array(comb)==1)[0].tolist()\n",
    "#             comb_seg_ele = ['segment' + str(c) for c in comb_number]\n",
    "#             comb_seg = eval('+'.join(comb_seg_ele))\n",
    "\n",
    "#             ct = np.array(list(cnt(comb_seg).values()))\n",
    "#             ct = ct.flatten()\n",
    "#             ct = np.unique(ct)\n",
    "            \n",
    "#             res.append(ct)\n",
    "\n",
    "#     res = np.array(res)\n",
    "#     res_shape = res.shape\n",
    "    \n",
    "#     # level이 달라서, flatten이 안되는 경우\n",
    "#     # -> 1행의 list로 변환\n",
    "#     if len(res_shape)==1:\n",
    "#         ret = []\n",
    "#         for r in res:\n",
    "#             for ele in r:\n",
    "#                 ret.append(ele)\n",
    "        \n",
    "#         res = ret\n",
    "        \n",
    "#     else:\n",
    "#         res = res.flatten()\n",
    "        \n",
    "#     # unique, sort, head\n",
    "#     res = np.unique(sorted(res))[:head]\n",
    "\n",
    "#     return(res)\n",
    "\n",
    "# print('  iter : min(1st, 2nd, 3rd, ...)')\n",
    "# print('-'*40)\n",
    "# for iter in range(1,3+1):\n",
    "#     print(f'     {iter} : {comb_seg(iter,head=7)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3442a8b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:51.005160Z",
     "start_time": "2022-01-06T18:55:50.991659Z"
    }
   },
   "outputs": [],
   "source": [
    "cnt(train10['cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76d0890",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "># **Category Level Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7925bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:51.025163Z",
     "start_time": "2022-01-06T18:55:51.007161Z"
    }
   },
   "outputs": [],
   "source": [
    "# 하나의 데이터셋에 대해서 level 개수를 search\n",
    "def check_category(data,col_types, ret=['dict','list']):\n",
    "\n",
    "    cols = list(set(data.columns) - set(['id','target']))\n",
    "\n",
    "    if ret=='dict':\n",
    "        len_cate = {}\n",
    "    elif ret=='list':\n",
    "        len_cate = []\n",
    "    else:\n",
    "        raise('error ret')\n",
    "        \n",
    "    for col in cols:\n",
    "        if col_types[col]==str:\n",
    "            _len = len(data[col].value_counts().index)\n",
    "            \n",
    "            if ret=='dict':\n",
    "                len_cate[col] = _len\n",
    "            elif ret=='list':\n",
    "                len_cate.append(_len)\n",
    "            \n",
    "    return(len_cate)\n",
    "\n",
    "check_category(train10,COL_TYPE,ret='dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37621b1b",
   "metadata": {},
   "source": [
    "##### train에서 모두 2개 이상으로, 이상없음\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60882259",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:51.050166Z",
     "start_time": "2022-01-06T18:55:51.026663Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 두개 데이터셋의 동일한 변수에 대해서, level이 같은지 확인\n",
    "def check_category2(data1,data2,col_types):\n",
    "\n",
    "    cols = list(set(data1.columns) - set(['id','target']))\n",
    "    max_char_len = max([len(x) if col_types[x]==str else 0 for x in col_types.keys()])\n",
    "    \n",
    "    # 없음\n",
    "    for col in cols:\n",
    "        \n",
    "        if col_types[col]==str:\n",
    "            data1_cate = data1[col].value_counts().index.values.sort_values().astype(str)\n",
    "            data2_cate = data2[col].value_counts().index.values.sort_values().astype(str)\n",
    "            \n",
    "            n_blank = (max_char_len-len(col))\n",
    "            if len(data1_cate)==len(data2_cate):\n",
    "                same_index =  data1_cate != data2_cate\n",
    "                print(col, ' '*n_blank, ':', same_index.sum())\n",
    "            else:\n",
    "                print(col, ' '*n_blank, ': differ length')\n",
    "            \n",
    "print(color.BOLD + color.BLUE + '> 다른 카테고리의 개수' + color.END)\n",
    "check_category2(train10,test10,COL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927410c4",
   "metadata": {},
   "source": [
    "##### 카테고리가 match 확인\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a9a97",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "># **교호작용항**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4cfa0e",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "make_scorer을 통해서 rmse로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066550d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:51.056667Z",
     "start_time": "2022-01-06T18:55:51.052166Z"
    }
   },
   "outputs": [],
   "source": [
    "def mape_fn(y_test, y_pred):\n",
    "    y_test, y_pred = np.array(y_test), np.array(y_pred)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    return mape\n",
    "\n",
    "def rmse_fn(y_test, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    return(rmse)\n",
    "\n",
    "def r2_fn(y_test, y_pred):\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a1f2e",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## without Interaction Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb65e6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:51.069669Z",
     "start_time": "2022-01-06T18:55:51.059167Z"
    }
   },
   "outputs": [],
   "source": [
    "if INTERACTION_CHECK:\n",
    "    automl_no_interaction = AUTOML_COMP(train = train10,\n",
    "                                        test = None,\n",
    "                                        target = 'body_mass',\n",
    "                                        col_types = COL_TYPE,\n",
    "                                        objective = 'regression',\n",
    "                                        ignore_features = ['id'],\n",
    "                                        missing_return = False,\n",
    "                                        scoring = make_scorer(rmse_fn,greater_is_better=False),\n",
    "                                        fit_model = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d25285",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## with Interaction Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abac84a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:51.082170Z",
     "start_time": "2022-01-06T18:55:51.071169Z"
    }
   },
   "outputs": [],
   "source": [
    "if INTERACTION_CHECK:\n",
    "\n",
    "    # interaction term 추가\n",
    "    interaction_df = interaction_term(train10.copy(),list(set(num_vari)-set(['body_mass'])))\n",
    "\n",
    "    # COL_TYPE에도 추가\n",
    "    COL_TYPE2 = COL_TYPE.copy()\n",
    "    for col in interaction_df.columns:\n",
    "        if col.find('*')>0:\n",
    "            COL_TYPE2[col] = int\n",
    "            \n",
    "    automl_interaction = AUTOML_COMP(train = interaction_df,\n",
    "                                     test = None,\n",
    "                                     target = 'body_mass',\n",
    "                                     col_types = COL_TYPE2,\n",
    "                                     objective = 'regression',\n",
    "                                     ignore_features = ['id'],\n",
    "                                     missing_return = False,\n",
    "                                     scoring = make_scorer(rmse_fn,greater_is_better=False),\n",
    "                                     fit_model = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e010106f",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0718991",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:51.093672Z",
     "start_time": "2022-01-06T18:55:51.083670Z"
    }
   },
   "outputs": [],
   "source": [
    "if INTERACTION_CHECK:\n",
    "    \n",
    "    comp_df = pd.DataFrame({\n",
    "        'model'          : [name for name,model in automl_no_interaction['model']],\n",
    "        'no_interaction' : [np.mean(res) for res in automl_no_interaction['cv_result']],\n",
    "        'interaction'    : [np.mean(res) for res in automl_interaction   ['cv_result']]\n",
    "    })\n",
    "    comp_df['res'] = comp_df['interaction'] - comp_df['no_interaction']\n",
    "\n",
    "    sort_var = ['interaction' if comp_df['interaction'].max() > comp_df['no_interaction'].max() else 'no_interaction']\n",
    "    comp_df  = comp_df.sort_values(sort_var,ascending = False)\n",
    "\n",
    "    print(round(comp_df,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d00fab6",
   "metadata": {},
   "source": [
    "LASSO,LR,KNN의 경우에는 교호작용의 효과가 안 좋고, LGBM, XGB는 교호작용의 효과가 좋음. 나머지는 비슷함\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c494aa4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:51.127176Z",
     "start_time": "2022-01-06T18:55:51.098172Z"
    }
   },
   "outputs": [],
   "source": [
    "if INTERACTION:\n",
    "    train11 = interaction_term(train10.copy() ,list(set(num_vari)-set(['body_mass'])))\n",
    "    test11  = interaction_term(test10 .copy() ,list(set(num_vari)-set(['body_mass'])))\n",
    "\n",
    "    for int_var in train11.columns[[col.find('*')>0 for col in train11.columns]]:\n",
    "        COL_TYPE[int_var] = int\n",
    "else:\n",
    "    train11 = train10.copy()\n",
    "    test11  = test10 .copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1354b1c7",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "여기까지 대략 14분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a7f70c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:51.143678Z",
     "start_time": "2022-01-06T18:55:51.129176Z"
    }
   },
   "outputs": [],
   "source": [
    "f'{(time.time()-query_start_time)/60:.1f} Mins'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d147d4c5",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "save/load session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd356ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:51.154679Z",
     "start_time": "2022-01-06T18:55:51.147678Z"
    }
   },
   "outputs": [],
   "source": [
    "# dill.dump_session('notebook_env.db')\n",
    "# dill.load_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04617ab",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "># **Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cf778b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:51.167181Z",
     "start_time": "2022-01-06T18:55:51.156180Z"
    }
   },
   "outputs": [],
   "source": [
    "scale_var = setdiff([col for col in COL_TYPE.keys() if COL_TYPE[col] in [int,float]],\n",
    "                    ['id','body_mass'])\n",
    "print(scale_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2fa6d3",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "scaling 대상 변수들의 boundary 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abda0ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:51.184183Z",
     "start_time": "2022-01-06T18:55:51.168681Z"
    }
   },
   "outputs": [],
   "source": [
    "if SCALE:\n",
    "\n",
    "    # len(scale_var),len(np.unique(scale_var)[0])\n",
    "\n",
    "    # 모두 0 이상의 값을 가짐\n",
    "    # min, max가 test에서 bound를 벗어나는게 있긴함..\n",
    "    max_char_len = max([len(x) for x in scale_var])\n",
    "    for iter,var in enumerate(scale_var):\n",
    "        n_blank = ' '*(max_char_len-len(var))\n",
    "        print(color.BOLD+color.BLUE+f'({iter+1}) {var} {n_blank}- '+color.END+\n",
    "              f'tr : {minmax(train11[var],1)}\\t, te : {minmax(test11[var],1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e287a",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a7f41e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:51.191184Z",
     "start_time": "2022-01-06T18:55:51.186183Z"
    }
   },
   "outputs": [],
   "source": [
    "if SCALE_CHECK:\n",
    "\n",
    "    automl_base = AUTOML_COMP(train = train11,\n",
    "                              test = None,\n",
    "                              target = 'body_mass',\n",
    "                              col_types = COL_TYPE,\n",
    "                              objective = 'regression',\n",
    "                              ignore_features = ['id'],\n",
    "                              missing_return = False,\n",
    "                              scoring = make_scorer(rmse_fn,greater_is_better=False),\n",
    "                              fit_model = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46465feb",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Normalization (MinMaxScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd7b422",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:51.204686Z",
     "start_time": "2022-01-06T18:55:51.193184Z"
    }
   },
   "outputs": [],
   "source": [
    "if SCALE_CHECK:\n",
    "\n",
    "    scale_df = train11.copy()\n",
    "    \n",
    "    # Normalization\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(scale_df[scale_var])\n",
    "\n",
    "    scale_df[scale_var] = scaler.transform(scale_df[scale_var])\n",
    "\n",
    "    automl_norm = AUTOML_COMP(train = scale_df,\n",
    "                              test = None,\n",
    "                              target = 'body_mass',\n",
    "                              col_types = COL_TYPE,\n",
    "                              objective = 'regression',\n",
    "                              ignore_features = ['id'],\n",
    "                              missing_return = False,\n",
    "                              scoring = make_scorer(rmse_fn,greater_is_better=False),\n",
    "                              fit_model = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17ea603",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Standardization (StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e1b4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:51.219188Z",
     "start_time": "2022-01-06T18:55:51.206186Z"
    }
   },
   "outputs": [],
   "source": [
    "if SCALE_CHECK:\n",
    "\n",
    "    scale_df = train11.copy()\n",
    "    \n",
    "    # Normalization\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(scale_df[scale_var])\n",
    "\n",
    "    scale_df[scale_var] = scaler.transform(scale_df[scale_var])\n",
    "\n",
    "    automl_std  = AUTOML_COMP(train = scale_df,\n",
    "                              test = None,\n",
    "                              target = 'body_mass',\n",
    "                              col_types = COL_TYPE,\n",
    "                              objective = 'regression',\n",
    "                              ignore_features = ['id'],\n",
    "                              missing_return = False,\n",
    "                              scoring = make_scorer(rmse_fn,greater_is_better=False),\n",
    "                              fit_model = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b11d46",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Robust (RobustScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec85e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:51.233689Z",
     "start_time": "2022-01-06T18:55:51.220688Z"
    }
   },
   "outputs": [],
   "source": [
    "if SCALE_CHECK:\n",
    "\n",
    "    scale_df = train11.copy()\n",
    "    \n",
    "    # Normalization\n",
    "    scaler = RobustScaler()\n",
    "    scaler.fit(scale_df[scale_var])\n",
    "\n",
    "    scale_df[scale_var] = scaler.transform(scale_df[scale_var])\n",
    "\n",
    "    automl_rob  = AUTOML_COMP(train = scale_df,\n",
    "                              test = None,\n",
    "                              target = 'body_mass',\n",
    "                              col_types = COL_TYPE,\n",
    "                              objective = 'regression',\n",
    "                              ignore_features = ['id'],\n",
    "                              missing_return = False,\n",
    "                              scoring = make_scorer(rmse_fn,greater_is_better=False),\n",
    "                              fit_model = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a2b36",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9250bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T11:41:05.074668Z",
     "start_time": "2022-01-06T11:41:05.067667Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8460d00f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:51.251192Z",
     "start_time": "2022-01-06T18:55:51.235190Z"
    }
   },
   "outputs": [],
   "source": [
    "if SCALE_CHECK:\n",
    "    \n",
    "    comp_df = pd.DataFrame({\n",
    "        'model' : [name for name,model in automl_base['model']],\n",
    "        'base'  : [np.mean(res) for res in automl_base['cv_result']],\n",
    "        'norm'  : [np.mean(res) for res in automl_norm['cv_result']],\n",
    "        'std'   : [np.mean(res) for res in automl_std ['cv_result']],\n",
    "        'rob'   : [np.mean(res) for res in automl_rob ['cv_result']],\n",
    "    })\n",
    "    comp_df['res1'] = comp_df['norm'] - comp_df['base']\n",
    "    comp_df['res2'] = comp_df['std' ] - comp_df['base']\n",
    "    comp_df['res3'] = comp_df['rob' ] - comp_df['base']\n",
    "    \n",
    "    comp_df['res_max'] = comp_df[['res1','res2','res3']].apply(lambda x: max(x), axis=1)\n",
    "    comp_df['res_min'] = comp_df[['res1','res2','res3']].apply(lambda x: min(x), axis=1)\n",
    "    comp_df = comp_df.drop(['res1','res2','res3'],axis=1)\n",
    "    \n",
    "    comp_df = comp_df.sort_values(['base'],ascending = False).reset_index(drop=True)\n",
    "\n",
    "    print(round(comp_df,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312bf85a",
   "metadata": {},
   "source": [
    "KNN, SVM의 경우에는 Scaling의 효과가 좋고, EN, CART의 경우에는 Scaling의 효과가 안 좋음. 나머지는 비슷함.\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb1d125",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:51.290697Z",
     "start_time": "2022-01-06T18:55:51.255192Z"
    }
   },
   "outputs": [],
   "source": [
    "if SCALE:\n",
    "\n",
    "    # Normalization\n",
    "    scaler = RobustScaler()\n",
    "    scaler.fit(train11[scale_var])\n",
    "    # print(scaler.n_samples_seen_, scaler.data_min_, scaler.data_max_, scaler.feature_range)\n",
    "\n",
    "    train12 = train11.copy()\n",
    "    test12  = test11 .copy()\n",
    "\n",
    "    train12[scale_var] = scaler.transform(train12[scale_var])\n",
    "    test12 [scale_var] = scaler.transform(test12 [scale_var])\n",
    "    \n",
    "    minmax_df = test12[scale_var].apply(lambda x: minmax(x))\n",
    "    minmax_df.index = ['min','max']\n",
    "    \n",
    "    print(color.BOLD + color.BLUE + '> Robust Scaling' + color.END)\n",
    "    print(minmax_df)\n",
    "    \n",
    "else:\n",
    "    train12 = train11.copy()\n",
    "    test12  = test11. copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca456318",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "># **Modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233309b6",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### Initial Value 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c101cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:51.304198Z",
     "start_time": "2022-01-06T18:55:51.293197Z"
    }
   },
   "outputs": [],
   "source": [
    "print('-'*50)\n",
    "print('   Initial Values')\n",
    "print('-'*50)\n",
    "max_char_len = max([len(var) for var in ini_var])\n",
    "for var in ini_var:\n",
    "    char_len = ' '*(max_char_len-len(var))\n",
    "    print(f'\\t{var} {char_len} : {eval(var)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5166e9f",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## AutoML Validation RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339ad4d4",
   "metadata": {},
   "source": [
    "모형 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3338a3e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:51.319700Z",
     "start_time": "2022-01-06T18:55:51.305698Z"
    }
   },
   "outputs": [],
   "source": [
    "models_dict={\n",
    "    'LR'    : LinearRegression(),\n",
    "    'RIDGE' : Ridge(random_state=SEED),\n",
    "    'LASSO' : Lasso(random_state=SEED),\n",
    "    'KNN'   : KNeighborsRegressor(),\n",
    "    'CART'  : DecisionTreeRegressor(random_state=SEED),\n",
    "    'EN'    : ElasticNet(random_state=SEED),\n",
    "    'SVM'   : SVR(),\n",
    "    'RFR'   : RandomForestRegressor(random_state=SEED),\n",
    "    'XGBR'  : XGBRegressor(iterations=10000,verbosity=0,random_state=SEED),\n",
    "    'LGBMR' : LGBMRegressor(random_state=SEED),\n",
    "    'AdaR'  : AdaBoostRegressor(random_state=SEED),\n",
    "    'Cat'   : CatBoostRegressor(iterations=10000,silent=True,random_state=SEED),\n",
    "}\n",
    "\n",
    "models=[]\n",
    "for name,model in models_dict.items():\n",
    "    model = (name,model)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0df3407",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c26d18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:55:51.347204Z",
     "start_time": "2022-01-06T18:55:51.321200Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train12.copy()\n",
    "for col in train_df.columns:\n",
    "    if COL_TYPE[col] not in [int,float]:\n",
    "        train_df[col].astype(str)\n",
    "\n",
    "train_df = pd.concat([\n",
    "    onehot_encoding(train_df.drop(['cluster'],axis=1),COL_TYPE),\n",
    "    train_df['cluster']\n",
    "],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3f9bc8",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Using Cluster as Segment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa0d25",
   "metadata": {},
   "source": [
    "모든 모형에 대한 Validation RMSE 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac44a88b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:57:07.809913Z",
     "start_time": "2022-01-06T18:56:04.064819Z"
    }
   },
   "outputs": [],
   "source": [
    "loss = []\n",
    "\n",
    "# with no interaction\n",
    "pbar = tqdm(models)\n",
    "for name,model in pbar:\n",
    "\n",
    "    pbar.set_description(f'fitting... ({name})')\n",
    "\n",
    "    test_fn = []\n",
    "    for value in train_df['cluster'].unique():\n",
    "\n",
    "        inter_tr_df = train_df[train_df['cluster']==value].drop(['id','cluster'],axis=1)\n",
    "        nointer_tr_df = inter_tr_df[setdiff([col for col in train_df.columns if col.find('*')<=0], ['id','cluster'])]\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(nointer_tr_df.drop(['body_mass'],axis=1), \n",
    "                                                            nointer_tr_df['body_mass'], \n",
    "                                                            test_size=0.3, \n",
    "                                                            shuffle=False, \n",
    "                                                            random_state=SEED)\n",
    "\n",
    "        #model = LinearRegression()\n",
    "        model.fit(X_train,y_train)\n",
    "\n",
    "        X_test['pred'] = model.predict(X_test)\n",
    "        X_test['body_mass'] = y_test\n",
    "\n",
    "        test_fn.append(X_test)\n",
    "        \n",
    "    res = pd.concat(test_fn,axis=0)[['body_mass','pred']]\n",
    "    rmse = rmse_fn(res['body_mass'],res['pred'])\n",
    "\n",
    "    loss.append((res, rmse))\n",
    "\n",
    "# with interaction\n",
    "pbar = tqdm(models)\n",
    "for name,model in pbar:\n",
    "\n",
    "    pbar.set_description(f'fitting... ({name})')\n",
    "\n",
    "    test_fn = []\n",
    "    for value in train_df['cluster'].unique():\n",
    "\n",
    "        inter_tr_df = train_df[train_df['cluster']==value].drop(['id','cluster'],axis=1)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(inter_tr_df.drop(['body_mass'],axis=1), \n",
    "                                                            inter_tr_df['body_mass'], \n",
    "                                                            test_size=0.3, \n",
    "                                                            shuffle=False, \n",
    "                                                            random_state=SEED)\n",
    "\n",
    "        #model = LinearRegression()\n",
    "        model.fit(X_train,y_train)\n",
    "\n",
    "        X_test['pred'] = model.predict(X_test)\n",
    "        X_test['body_mass'] = y_test\n",
    "\n",
    "        test_fn.append(X_test)\n",
    "        \n",
    "    res = pd.concat(test_fn,axis=0)[['body_mass','pred']]\n",
    "    rmse = rmse_fn(res['body_mass'],res['pred'])\n",
    "\n",
    "    loss.append((res, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f437eaf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:57:07.820415Z",
     "start_time": "2022-01-06T18:57:07.811914Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# validation rmse\n",
    "val_rmse_df_1 = pd.DataFrame({\n",
    "    'model' : [name+'_nointer' for name,model in models]+[name+'_inter' for name,model in models],\n",
    "    'rmse'  : [rmse for _, rmse in loss]\n",
    "})\n",
    "\n",
    "val_rmse_df_1.sort_values('rmse',ascending=True,inplace=True)\n",
    "\n",
    "# 상위 5개\n",
    "val_rmse_df_1 = val_rmse_df_1#[:5]\n",
    "\n",
    "val_rmse_df_1['ratio'] = (1 / val_rmse_df_1['rmse']) / sum(1 / val_rmse_df_1['rmse'])\n",
    "val_rmse_df_1.set_index('model',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eced67c",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Using Cluster as Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eb553f",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "모든 모형에 대한 Validation RMSE 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ef949d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:57:47.499953Z",
     "start_time": "2022-01-06T18:57:07.821915Z"
    }
   },
   "outputs": [],
   "source": [
    "loss = []\n",
    "\n",
    "# with no interaction\n",
    "pbar = tqdm(models)\n",
    "for name,model in pbar:\n",
    "\n",
    "    pbar.set_description(f'fitting... ({name})')\n",
    "\n",
    "    inter_tr_df = train_df.drop(['id','cluster'],axis=1)\n",
    "    nointer_tr_df = inter_tr_df[setdiff([col for col in train_df.columns if col.find('*')<=0], ['id','cluster'])]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(nointer_tr_df.drop(['body_mass'],axis=1), \n",
    "                                                        nointer_tr_df['body_mass'], \n",
    "                                                        test_size=0.3, \n",
    "                                                        shuffle=False, \n",
    "                                                        random_state=SEED)\n",
    "\n",
    "    #model = LinearRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    X_test['pred'] = model.predict(X_test)\n",
    "    X_test['body_mass'] = y_test\n",
    "\n",
    "\n",
    "    res = X_test[['body_mass','pred']]\n",
    "    rmse = rmse_fn(res['body_mass'],res['pred'])\n",
    "\n",
    "    loss.append((res, rmse))\n",
    "    \n",
    "# with interaction\n",
    "pbar = tqdm(models)\n",
    "for name,model in pbar:\n",
    "\n",
    "    pbar.set_description(f'fitting... ({name})')\n",
    "\n",
    "    inter_tr_df = train_df.drop(['id','cluster'],axis=1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(inter_tr_df.drop(['body_mass'],axis=1), \n",
    "                                                        inter_tr_df['body_mass'], \n",
    "                                                        test_size=0.3, \n",
    "                                                        shuffle=False, \n",
    "                                                        random_state=SEED)\n",
    "\n",
    "    #model = LinearRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    X_test['pred'] = model.predict(X_test)\n",
    "    X_test['body_mass'] = y_test\n",
    "\n",
    "\n",
    "    res = X_test[['body_mass','pred']]\n",
    "    rmse = rmse_fn(res['body_mass'],res['pred'])\n",
    "\n",
    "    loss.append((res, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc711a57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:57:47.510455Z",
     "start_time": "2022-01-06T18:57:47.502454Z"
    }
   },
   "outputs": [],
   "source": [
    "# validation rmse\n",
    "val_rmse_df_2 = pd.DataFrame({\n",
    "    'model' : [name+'_nointer' for name,model in models]+[name+'_inter' for name,model in models],\n",
    "    'rmse'  : [rmse for _, rmse in loss]\n",
    "})\n",
    "\n",
    "val_rmse_df_2.sort_values('rmse',ascending=True,inplace=True)\n",
    "\n",
    "# 상위 5개\n",
    "val_rmse_df_2 = val_rmse_df_2#[:5]\n",
    "\n",
    "val_rmse_df_2['ratio'] = (1 / val_rmse_df_2['rmse']) / sum(1 / val_rmse_df_2['rmse'])\n",
    "val_rmse_df_2.set_index('model',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04d94a6",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Cluster Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d84f05e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:57:47.531457Z",
     "start_time": "2022-01-06T18:57:47.511955Z"
    }
   },
   "outputs": [],
   "source": [
    "a1 = val_rmse_df_1.copy()\n",
    "a2 = val_rmse_df_2.copy()\n",
    "\n",
    "a1.index = '1_' + a1.index\n",
    "a2.index = '2_' + a2.index\n",
    "\n",
    "a = pd.concat([\n",
    "    a1,\n",
    "    a2,\n",
    "],axis=0).sort_values('rmse')\n",
    "\n",
    "a[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60bc79",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Linear Regression with Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afdfa1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:58:45.189779Z",
     "start_time": "2022-01-06T18:58:45.136272Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train12.copy()\n",
    "for col in train_df.columns:\n",
    "    if COL_TYPE[col] not in [int,float]:\n",
    "        train_df[col].astype(str)\n",
    "        \n",
    "test_df  = test12.copy()\n",
    "for col in test_df.columns:\n",
    "    if COL_TYPE[col] not in [int,float]:\n",
    "        test_df[col].astype(str)\n",
    "\n",
    "train_df = pd.concat([\n",
    "    onehot_encoding(train_df.drop(['cluster'],axis=1),COL_TYPE),\n",
    "    train_df['cluster']\n",
    "],axis=1)\n",
    "\n",
    "test_df  = pd.concat([\n",
    "    onehot_encoding(test_df.drop(['cluster'],axis=1),COL_TYPE),\n",
    "    test_df['cluster']\n",
    "],axis=1)\n",
    "\n",
    "train_fn = []\n",
    "test_fn  = []\n",
    "for value in train_df['cluster'].unique():\n",
    "    \n",
    "    tr_df = train_df[train_df['cluster']==value]\n",
    "    te_df = test_df [test_df ['cluster']==value]\n",
    "\n",
    "    model = models_dict['LR']\n",
    "    model.fit(tr_df.drop(['id','body_mass','cluster'],axis=1), \n",
    "              tr_df['body_mass'])\n",
    "\n",
    "    te_df['Body Mass (g)'] = model.predict(te_df.drop(['id','cluster'],axis=1))\n",
    "    tr_df['pred']          = model.predict(tr_df.drop(['id','body_mass','cluster'],axis=1))\n",
    "    \n",
    "    train_fn.append(tr_df)\n",
    "    test_fn.append(te_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e33973",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:58:46.355927Z",
     "start_time": "2022-01-06T18:58:46.343425Z"
    }
   },
   "outputs": [],
   "source": [
    "train_fn = pd.concat(train_fn,axis=0)[['body_mass','pred','cluster']]\n",
    "rmse_fn(train_fn['body_mass'].values,train_fn['pred'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e40ba8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:58:48.201661Z",
     "start_time": "2022-01-06T18:58:47.591084Z"
    }
   },
   "outputs": [],
   "source": [
    "_max = max(train_fn['pred'].max(),train_fn['body_mass'].max())\n",
    "_min = min(train_fn['pred'].min(),train_fn['body_mass'].min())\n",
    "\n",
    "sns.scatterplot(train_fn['pred'],train_fn['body_mass'],hue=train_fn['cluster'])\n",
    "plt.axline((_min,_min),(_max,_max),color='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4e90f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:58:50.897004Z",
     "start_time": "2022-01-06T18:58:50.887502Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_fn = pd.concat(test_fn,axis=0)[['id','Body Mass (g)']]\n",
    "sub_fn.sort_index(inplace=True)\n",
    "\n",
    "sub_fn.to_csv(OUT_PATH + 'out_lr_cluster_nointer.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0318ae20",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## LASSO Regression with Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ec46bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:58:52.421197Z",
     "start_time": "2022-01-06T18:58:52.366190Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train12.copy()\n",
    "for col in train_df.columns:\n",
    "    if COL_TYPE[col] not in [int,float]:\n",
    "        train_df[col].astype(str)\n",
    "        \n",
    "test_df  = test12.copy()\n",
    "for col in test_df.columns:\n",
    "    if COL_TYPE[col] not in [int,float]:\n",
    "        test_df[col].astype(str)\n",
    "\n",
    "train_df = pd.concat([\n",
    "    onehot_encoding(train_df.drop(['cluster'],axis=1),COL_TYPE),\n",
    "    train_df['cluster']\n",
    "],axis=1)\n",
    "\n",
    "test_df  = pd.concat([\n",
    "    onehot_encoding(test_df.drop(['cluster'],axis=1),COL_TYPE),\n",
    "    test_df['cluster']\n",
    "],axis=1)\n",
    "\n",
    "train_fn = []\n",
    "test_fn  = []\n",
    "for value in train_df['cluster'].unique():\n",
    "    \n",
    "    tr_df = train_df[train_df['cluster']==value]\n",
    "    te_df = test_df [test_df ['cluster']==value]\n",
    "\n",
    "    model = models_dict['LASSO']\n",
    "    model.fit(tr_df.drop(['id','body_mass','cluster'],axis=1), \n",
    "              tr_df['body_mass'])\n",
    "\n",
    "    te_df['Body Mass (g)'] = model.predict(te_df.drop(['id','cluster'],axis=1))\n",
    "    tr_df['pred']          = model.predict(tr_df.drop(['id','body_mass','cluster'],axis=1))\n",
    "    \n",
    "    train_fn.append(tr_df)\n",
    "    test_fn.append(te_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefac448",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:58:54.015400Z",
     "start_time": "2022-01-06T18:58:54.007399Z"
    }
   },
   "outputs": [],
   "source": [
    "train_fn = pd.concat(train_fn,axis=0)[['body_mass','pred','cluster']]\n",
    "rmse_fn(train_fn['body_mass'].values,train_fn['pred'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1943d3b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:58:55.798626Z",
     "start_time": "2022-01-06T18:58:55.343068Z"
    }
   },
   "outputs": [],
   "source": [
    "_max = max(train_fn['pred'].max(),train_fn['body_mass'].max())\n",
    "_min = min(train_fn['pred'].min(),train_fn['body_mass'].min())\n",
    "\n",
    "sns.scatterplot(train_fn['pred'],train_fn['body_mass'],hue=train_fn['cluster'])\n",
    "plt.axline((_min,_min),(_max,_max),color='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df912bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:58:59.060040Z",
     "start_time": "2022-01-06T18:58:59.048539Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_fn = pd.concat(test_fn,axis=0)[['id','Body Mass (g)']]\n",
    "sub_fn.sort_index(inplace=True)\n",
    "\n",
    "sub_fn.to_csv(OUT_PATH + 'out_lasso_cluster_nointer.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465aacd4",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Ensemble with Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8601f046",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T18:59:01.377835Z",
     "start_time": "2022-01-06T18:59:01.344330Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train12.copy()\n",
    "for col in train_df.columns:\n",
    "    if COL_TYPE[col] not in [int,float]:\n",
    "        train_df[col].astype(str)\n",
    "        \n",
    "test_df  = test12.copy()\n",
    "for col in test_df.columns:\n",
    "    if COL_TYPE[col] not in [int,float]:\n",
    "        test_df[col].astype(str)\n",
    "\n",
    "train_df = pd.concat([\n",
    "    onehot_encoding(train_df.drop(['cluster'],axis=1),COL_TYPE),\n",
    "    train_df['cluster']\n",
    "],axis=1)\n",
    "\n",
    "test_df  = pd.concat([\n",
    "    onehot_encoding(test_df.drop(['cluster'],axis=1),COL_TYPE),\n",
    "    test_df['cluster']\n",
    "],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a5794e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T19:00:47.691835Z",
     "start_time": "2022-01-06T18:59:03.023044Z"
    }
   },
   "outputs": [],
   "source": [
    "# (1) cluster as segment\n",
    "train_fn, test_fn = [],[]\n",
    "for value in train_df['cluster'].unique():\n",
    "    \n",
    "    inter_tr_df = train_df[train_df['cluster']==value]\n",
    "    inter_te_df = test_df [test_df ['cluster']==value]\n",
    "    \n",
    "    no_inter_tr_df = inter_tr_df[[col for col in inter_tr_df.columns if col.find('*')<0]]\n",
    "    no_inter_te_df = inter_te_df[[col for col in inter_te_df.columns if col.find('*')<0]]\n",
    "    \n",
    "    tr_raw_df = inter_tr_df\n",
    "    te_raw_df = inter_te_df\n",
    "\n",
    "    pbar = tqdm(models)\n",
    "    for name,model in pbar:\n",
    "        pbar.set_description(f'fitting... ({name})')\n",
    "    \n",
    "        model_inter   = model\n",
    "        model_nointer = model\n",
    "        \n",
    "        # interaction\n",
    "        model_inter.fit(inter_tr_df.drop(['id','body_mass','cluster'],axis=1), inter_tr_df['body_mass'])\n",
    "        \n",
    "        tr_raw_df[f'1_pred_{name}_inter'] = model_inter.predict(inter_tr_df.drop(['id','body_mass','cluster'],axis=1))\n",
    "        te_raw_df[f'1_pred_{name}_inter'] = model_inter.predict(inter_te_df.drop(['id','cluster'],axis=1))\n",
    "        \n",
    "        # no intercation\n",
    "        model_nointer.fit(no_inter_tr_df.drop(['id','body_mass','cluster'],axis=1), no_inter_tr_df['body_mass'])\n",
    "        \n",
    "        tr_raw_df[f'1_pred_{name}_nointer'] = model_nointer.predict(no_inter_tr_df.drop(['id','body_mass','cluster'],axis=1))\n",
    "        te_raw_df[f'1_pred_{name}_nointer'] = model_nointer.predict(no_inter_te_df.drop(['id','cluster'],axis=1))\n",
    "\n",
    "    train_fn.append(tr_raw_df)\n",
    "    test_fn .append(te_raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba63fb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T19:01:46.924856Z",
     "start_time": "2022-01-06T19:00:47.693835Z"
    }
   },
   "outputs": [],
   "source": [
    "# (2) cluster as variable\n",
    "inter_tr_df = train_df.copy()\n",
    "inter_te_df = test_df .copy()\n",
    "\n",
    "no_inter_tr_df = inter_tr_df[[col for col in inter_tr_df.columns if col.find('*')<0]]\n",
    "no_inter_te_df = inter_te_df[[col for col in inter_te_df.columns if col.find('*')<0]]\n",
    "\n",
    "tr_raw_df = train_df.copy()\n",
    "te_raw_df = test_df .copy()\n",
    "\n",
    "pbar = tqdm(models)\n",
    "for name,model in pbar:\n",
    "    pbar.set_description(f'fitting... ({name})')\n",
    "\n",
    "    model_inter   = model\n",
    "    model_nointer = model\n",
    "\n",
    "    # interaction\n",
    "    model_inter.fit(inter_tr_df.drop(['id','body_mass','cluster'],axis=1), inter_tr_df['body_mass'])\n",
    "\n",
    "    tr_raw_df[f'2_pred_{name}_inter'] = model_inter.predict(inter_tr_df.drop(['id','body_mass','cluster'],axis=1))\n",
    "    te_raw_df[f'2_pred_{name}_inter'] = model_inter.predict(inter_te_df.drop(['id','cluster'],axis=1))\n",
    "\n",
    "    # no intercation\n",
    "    model_nointer.fit(no_inter_tr_df.drop(['id','body_mass','cluster'],axis=1), no_inter_tr_df['body_mass'])\n",
    "\n",
    "    tr_raw_df[f'2_pred_{name}_nointer'] = model_nointer.predict(no_inter_tr_df.drop(['id','body_mass','cluster'],axis=1))\n",
    "    te_raw_df[f'2_pred_{name}_nointer'] = model_nointer.predict(no_inter_te_df.drop(['id','cluster'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5f0202",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T19:03:59.508192Z",
     "start_time": "2022-01-06T19:03:59.498191Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1_ : as segment, _2 : as variable\n",
    "b = a[:5]\n",
    "b['ratio'] = 1/b['rmse']\n",
    "b['ratio'] = b['ratio'] / sum(b['ratio'])\n",
    "\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca40f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T19:04:59.195272Z",
     "start_time": "2022-01-06T19:04:59.173269Z"
    }
   },
   "outputs": [],
   "source": [
    "tr_ensemble_df = pd.concat([\n",
    "    pd.concat(train_fn,axis=0).reset_index(drop=True),\n",
    "    tr_raw_df[[col for col in tr_raw_df.columns if col.find('2_')>=0]].reset_index(drop=True)\n",
    "],axis=1)\n",
    "\n",
    "target_col = [col for col in tr_ensemble_df.columns if (col.find('1_')>=0) or (col.find('2_')>=0) or (col=='body_mass') or (col=='cluster')]\n",
    "\n",
    "tr_ensemble_df['pred_ensemble'] = tr_ensemble_df[['1_pred_LASSO_inter','1_pred_LR_nointer','1_pred_LASSO_nointer','1_pred_LR_nointer','1_pred_RIDGE_nointer']].apply(\n",
    "    #lambda x:sum(x)/5,\n",
    "    lambda x: x[0]*b['ratio'][b.index[0]]+\\\n",
    "              x[1]*b['ratio'][b.index[1]]+\\\n",
    "              x[2]*b['ratio'][b.index[2]]+\\\n",
    "              x[3]*b['ratio'][b.index[3]]+\\\n",
    "              x[4]*b['ratio'][b.index[4]],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "rmse_fn(tr_ensemble_df['body_mass'],tr_ensemble_df['pred_ensemble'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ebcdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T19:05:01.168022Z",
     "start_time": "2022-01-06T19:05:00.727466Z"
    }
   },
   "outputs": [],
   "source": [
    "_max = max(tr_ensemble_df['pred_ensemble'].max(),tr_ensemble_df['body_mass'].max())\n",
    "_min = min(tr_ensemble_df['pred_ensemble'].min(),tr_ensemble_df['body_mass'].min())\n",
    "\n",
    "sns.scatterplot(tr_ensemble_df['pred_ensemble'],tr_ensemble_df['body_mass'],hue=tr_ensemble_df['cluster'])\n",
    "plt.axline((_min,_min),(_max,_max),color='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a116b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T19:05:07.636344Z",
     "start_time": "2022-01-06T19:05:07.623342Z"
    }
   },
   "outputs": [],
   "source": [
    "te_ensemble_df = pd.concat([\n",
    "    pd.concat(test_fn,axis=0).reset_index(drop=True),\n",
    "    te_raw_df[[col for col in te_raw_df.columns if col.find('2_')>=0]].reset_index(drop=True)\n",
    "],axis=1)\n",
    "\n",
    "te_ensemble_df['pred_ensemble'] = te_ensemble_df[['1_pred_LASSO_nointer','1_pred_LR_nointer','1_pred_EN_inter']].apply(\n",
    "    lambda x:sum(x)/3,\n",
    "#     lambda x: x[0]*b['ratio'][b.index[0]]+\\\n",
    "#               x[1]*b['ratio'][b.index[1]]+\\\n",
    "#               x[2]*b['ratio'][b.index[2]],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1920d42c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T19:05:16.981030Z",
     "start_time": "2022-01-06T19:05:16.970029Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_fn = te_ensemble_df[['id','pred_ensemble']]\n",
    "sub_fn.sort_index(inplace=True)\n",
    "\n",
    "sub_fn.rename(columns = {'pred_ensemble' : 'Body Mass (g)'}, inplace = True)\n",
    "\n",
    "sub_fn\n",
    "\n",
    "sub_fn.to_csv(OUT_PATH + 'out_scaled_weighted_ensemble_cluster_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e70e272",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T16:35:02.159293Z",
     "start_time": "2022-01-06T16:35:02.153292Z"
    }
   },
   "outputs": [],
   "source": [
    "# rmse_fn(train_fn['body_mass'].values,train_fn['pred_ensemble'].values)\n",
    "\n",
    "pred = train_fn['pred_ensemble']\n",
    "\n",
    "# pred = \\\n",
    "# train_fn['pred_lr']   /3+\\\n",
    "# train_fn['pred_ridge']/3+\\\n",
    "# train_fn['pred_lasso']/3\n",
    "\n",
    "rmse_fn(train_fn['body_mass'],pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c944c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T16:35:04.633607Z",
     "start_time": "2022-01-06T16:35:04.178050Z"
    }
   },
   "outputs": [],
   "source": [
    "_max = max(pred.max(),train_fn['body_mass'].max())\n",
    "_min = min(pred.min(),train_fn['body_mass'].min())\n",
    "\n",
    "sns.scatterplot(pred,train_fn['body_mass'],hue=train_fn['cluster'])\n",
    "plt.axline((_min,_min),(_max,_max),color='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8aaa3e",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## LGBM setting with Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac126d0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T15:54:03.037024Z",
     "start_time": "2022-01-06T15:54:03.023023Z"
    }
   },
   "outputs": [],
   "source": [
    "# bayesian optimization에 쓰일 hyper parameter들의 boundary\n",
    "bounds_LGB = {\n",
    "    'num_leaves': (100, 800), \n",
    "    'min_data_in_leaf': (0, 150),\n",
    "    'bagging_fraction' : (0.3, 0.9),\n",
    "    'feature_fraction' : (0.3, 0.9),\n",
    "    'min_child_weight': (0.01, 1.),   \n",
    "    'reg_alpha': (0.01, 1.), \n",
    "    'reg_lambda': (0.01, 1),\n",
    "    'max_depth':(6, 23),\n",
    "    'learning_rate': (1e-8, 0.05),\n",
    "}\n",
    "\n",
    "# bayesian optimazation을 통하여 hyper parameter를 선택한\n",
    "# lightgbm modelling\n",
    "def build_lgb(x, y, val_x, val_y,\n",
    "              init_points=INIT_POINTS, n_iter=N_ITER, cv=N_CV, \n",
    "              ret_param=True, verbose=-1, is_test=False, \n",
    "              SEED=SEED, objective = OBJECTIVE):\n",
    "    \n",
    "    # verbose : 2 항상 출력, verbose = 1 최댓값일 때 출력, verbose = 0 출력 안함\n",
    "    \n",
    "    # set reg options\n",
    "    if objective=='regression':\n",
    "        ML = lgb.LGBMRegressor\n",
    "    elif objective=='binary':\n",
    "        ML = lgb.LGBMClassifier\n",
    "    \n",
    "    # (1) 각 hyper parameter들의 lgb model의 f1 score를 return\n",
    "    def LGB_bayesian(\n",
    "        num_leaves, \n",
    "        bagging_fraction,\n",
    "        feature_fraction,\n",
    "        min_child_weight, \n",
    "        min_data_in_leaf,\n",
    "        max_depth,\n",
    "        reg_alpha,\n",
    "        reg_lambda,\n",
    "        learning_rate,\n",
    "        objective = OBJECTIVE\n",
    "        ):\n",
    "        # LightGBM expects next three parameters need to be integer. \n",
    "        num_leaves       = int(num_leaves)\n",
    "        min_data_in_leaf = int(min_data_in_leaf)\n",
    "        max_depth        = int(max_depth)\n",
    "\n",
    "        assert type(num_leaves)       == int\n",
    "        assert type(min_data_in_leaf) == int\n",
    "        assert type(max_depth)        == int\n",
    "\n",
    "        params = {\n",
    "            'num_leaves': num_leaves, \n",
    "            'min_data_in_leaf': min_data_in_leaf,\n",
    "            'min_child_weight': min_child_weight,\n",
    "            'bagging_fraction' : bagging_fraction,\n",
    "            'feature_fraction' : feature_fraction,\n",
    "            'learning_rate' : learning_rate,\n",
    "            'max_depth': max_depth,\n",
    "            'reg_alpha': reg_alpha,\n",
    "            'reg_lambda': reg_lambda,\n",
    "            'objective': objective,\n",
    "            'save_binary': True,\n",
    "            'seed': SEED,\n",
    "            'feature_fraction_seed': SEED,\n",
    "            'bagging_seed': SEED,\n",
    "            'drop_seed': SEED,\n",
    "            'data_random_seed': SEED,\n",
    "            'boosting': 'gbdt', \n",
    "            'verbose': -1,\n",
    "            'boost_from_average': True,\n",
    "            'metric':METRIC,\n",
    "            'n_estimators': N_ESTIMATORS, # 1000\n",
    "            'n_jobs': -1,\n",
    "        }    \n",
    "        \n",
    "        model = ML(**params)\n",
    "        model.fit(x, y, eval_set=(val_x, val_y), early_stopping_rounds=30, verbose=-1)\n",
    "        pred = model.predict(val_x)\n",
    "        score = rmse_fn(val_y, pred)\n",
    "        return -score\n",
    "    \n",
    "    # (2) Get hyper parameter by bayesian optimazation\n",
    "    optimizer = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=SEED, verbose=-1)\n",
    "    \n",
    "    # initial point, n_iter에 대해서 maximize 하는 bayesian optimazation 실행\n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iter)#, acq='ei', xi=0.01)\n",
    "    # init_points는 처음 탐색 횟수. \n",
    "    # pbound에서 설정한 구간 내에서 init_points 만큼 입력값을 샘플링하여 계산이 진행\n",
    "    # n_iter은 연산 횟수입니다. 따라서 총 25번을 수행\n",
    "    # xi는 exploration-explotation의 강도를 조절하는 인수로 일반적으로 0.01로 설정하여 exploration을 높여줌\n",
    "    \n",
    "    \n",
    "    # (3) bayesian optimazation를 통해서 얻은 hyper parameter\n",
    "    param_lgb = {\n",
    "        'min_data_in_leaf': int(optimizer.max['params']['min_data_in_leaf']), \n",
    "        'num_leaves': int(optimizer.max['params']['num_leaves']), \n",
    "        'learning_rate': optimizer.max['params']['learning_rate'],\n",
    "        'min_child_weight': optimizer.max['params']['min_child_weight'],\n",
    "        'bagging_fraction': optimizer.max['params']['bagging_fraction'], \n",
    "        'feature_fraction': optimizer.max['params']['feature_fraction'],\n",
    "        'reg_lambda': optimizer.max['params']['reg_lambda'],\n",
    "        'reg_alpha': optimizer.max['params']['reg_alpha'],\n",
    "        'max_depth': int(optimizer.max['params']['max_depth']), \n",
    "        'objective': objective,\n",
    "        #'save_binary': True,\n",
    "        'seed': SEED,\n",
    "        'feature_fraction_seed': SEED,\n",
    "        'bagging_seed': SEED,\n",
    "        'drop_seed': SEED,\n",
    "        'data_random_seed': SEED,\n",
    "        'boosting': 'gbdt', \n",
    "        'verbose': -1,\n",
    "        'boost_from_average': True,\n",
    "        'metric': METRIC, #'auc',\n",
    "        'n_estimators': N_ESTIMATORS, # 1000\n",
    "        'n_jobs': -1,\n",
    "    }\n",
    "\n",
    "    # final parameter\n",
    "    params = param_lgb.copy()\n",
    "    \n",
    "    # final model\n",
    "    model = ML(**params)\n",
    "    model.fit(x, y, eval_set=(val_x, val_y), early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "              callbacks = [lgb.early_stopping(10, verbose=-1), lgb.log_evaluation(period=-1)])\n",
    "    \n",
    "    if ret_param:\n",
    "        return model, params\n",
    "    else:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a6b3e3",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### segment and dataset setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a523a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T15:44:30.723850Z",
     "start_time": "2022-01-06T15:44:30.718349Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ca==cp > exang > slope > sex\n",
    "# > ca는 2,3건수가 너무적음\n",
    "# > cp는 1,3건수가 너무 적음\n",
    "seg_var = ['cluster']\n",
    "\n",
    "# 각 세그별 최소 건수\n",
    "#[train12[seg_var_x].value_counts().min() for seg_var_x in seg_var]\n",
    "cnt(train12['cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdffdab6",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## LGBM fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d0bfa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T15:54:09.113796Z",
     "start_time": "2022-01-06T15:54:09.102294Z"
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "\n",
    "def objective(trial, _X, _y, objective = 'regression'):\n",
    "    \n",
    "    N_SPLIT = 5\n",
    "    \n",
    "    # set reg options\n",
    "    if objective=='regression':\n",
    "        ML = lgb.LGBMRegressor\n",
    "    elif objective=='binary':\n",
    "        ML = lgb.LGBMClassifier\n",
    "    \n",
    "    # optuna hyper-parameter grid\n",
    "    param_grid = {\n",
    "        # \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-8, 1e-2),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 5, 100, step=5),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 30, step=2),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 50, 1000, step=50),\n",
    "        'lambda_l1': trial.suggest_int('lambda_l1', 0, 100, step=5),\n",
    "        'lambda_l2': trial.suggest_int('lambda_l2', 0, 100, step=5),\n",
    "        'min_gain_to_split': trial.suggest_float('min_gain_to_split', 1, 30, step=3),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.1, 0.99, step=0.1),\n",
    "        'bagging_freq': trial.suggest_categorical('bagging_freq',[1]),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.1, 0.99, step=0.1),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 3000, step=5),\n",
    "        \n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'seed': SEED,\n",
    "        'feature_fraction_seed': SEED,\n",
    "        'bagging_seed': SEED,\n",
    "        'drop_seed': SEED,\n",
    "        'data_random_seed': SEED,\n",
    "        \n",
    "        'boosting': 'gbdt', \n",
    "        'verbose': -1,\n",
    "        #'verbose_eval': -1,\n",
    "        'boost_from_average': True,\n",
    "        'n_jobs': -1,\n",
    "        \n",
    "        'objective' : OBJECTIVE,\n",
    "    }\n",
    "\n",
    "    # cross validation\n",
    "    cv = StratifiedKFold(n_splits=N_SPLIT, shuffle=False)#, random_state=SEED)\n",
    "\n",
    "    # cross validation score\n",
    "    cv_scores = np.empty(N_SPLIT)\n",
    "    \n",
    "    # cv를 통해서 lgb적합해서 최적의 hyper-parameter 확인\n",
    "    for idx, (_train_idx, _test_idx) in enumerate(cv.split(_X, _y)):\n",
    "        _X_train, _X_test = _X.iloc[_train_idx], _X.iloc[_test_idx]\n",
    "        _y_train, _y_test = _y[_train_idx], _y[_test_idx]\n",
    "\n",
    "        model = ML(**param_grid)\n",
    "        model.fit(\n",
    "            _X_train,\n",
    "            _y_train,\n",
    "            verbose = -1,\n",
    "            eval_set=[(_X_test, _y_test)],\n",
    "            eval_metric='rmse',\n",
    "            early_stopping_rounds=30,\n",
    "            callbacks=[\n",
    "                LightGBMPruningCallback(trial, 'rmse')\n",
    "            ],  # Add a pruning callback\n",
    "        )\n",
    "        _preds = model.predict(_X_test)\n",
    "        cv_scores[idx] = rmse_fn(_y_test, _preds)\n",
    "\n",
    "    #return np.mean(f1_scores)\n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db340269",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T15:54:27.569640Z",
     "start_time": "2022-01-06T15:54:11.466095Z"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_df = train12.copy()\n",
    "test_df  = test12 .copy()\n",
    "\n",
    "# set reg options\n",
    "if OBJECTIVE=='regression':\n",
    "    ML = lgb.LGBMRegressor\n",
    "elif OBJECTIVE=='binary':\n",
    "    ML = lgb.LGBMClassifier\n",
    "\n",
    "for seg_var_x in seg_var:\n",
    "\n",
    "    train_df[seg_var_x+'_pred'] = np.nan\n",
    "    test_df [seg_var_x+'_pred'] = np.nan\n",
    "\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    # remove needless variable\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    drop_var = ['id','body_mass'] +\\\n",
    "    [seg_var_x           for seg_var_x in seg_var] +\\\n",
    "    [seg_var_x+'_pred'   for seg_var_x in seg_var]\n",
    "\n",
    "    X_train = train_df[list(set(train_df.columns)-set(drop_var))]\n",
    "    X_test  = test_df [list(set(test_df.columns) -set(drop_var))]\n",
    "\n",
    "    y_train = train_df['body_mass'].astype(float).values\n",
    "\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    # feature importance\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    model = ML(seed = SEED)\n",
    "    model.fit(X_train, y_train, verbose=-1)\n",
    "\n",
    "    feature_imp = pd.DataFrame(zip(X_train.columns,\n",
    "                                   model.feature_importances_.astype(float)), \n",
    "                               columns=['feature','imp']).sort_values(by='imp')\n",
    "\n",
    "    reduced_var = list(feature_imp.feature[feature_imp.imp>1])\n",
    "\n",
    "    X_train_new = X_train[reduced_var]\n",
    "    X_test_new  = X_test [reduced_var]\n",
    "    \n",
    "    del model\n",
    "\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    # modelling\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    study = optuna.create_study(direction='minimize', study_name='LGBM Regressor')\n",
    "    func = lambda trial: objective(trial, X_train_new, y_train)\n",
    "    study.optimize(func, n_trials=20)\n",
    "\n",
    "    model = ML(**study.best_params)\n",
    "    model.fit(X_train_new,y_train)\n",
    "\n",
    "    # seg별 predict값 넣기\n",
    "    train_df[seg_var_x+'_pred'] = model.predict(X_train_new)\n",
    "    test_df [seg_var_x+'_pred'] = model.predict(X_test_new)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ade336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T15:54:27.587142Z",
     "start_time": "2022-01-06T15:54:27.572140Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse_fn(train_df['cluster_pred'],train_df['body_mass'])\n",
    "train_df['cluster_pred'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27ee0f7",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Cluster 포함 전체 Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f802373f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T16:17:06.734232Z",
     "start_time": "2022-01-06T16:15:33.273864Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_df = train12.copy()\n",
    "test_df  = test12 .copy()\n",
    "\n",
    "for seg_var_x in seg_var:\n",
    "\n",
    "    train_df[seg_var_x+'_pred'] = np.nan\n",
    "    test_df [seg_var_x+'_pred'] = np.nan\n",
    "\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    # remove needless variable\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    drop_var = ['id','body_mass'] +\\\n",
    "    [seg_var_x           for seg_var_x in seg_var] +\\\n",
    "    [seg_var_x+'_pred'   for seg_var_x in seg_var]\n",
    "\n",
    "    X_train = train_df[list(set(train_df.columns)-set(drop_var))]\n",
    "    X_test  = test_df [list(set(test_df.columns) -set(drop_var))]\n",
    "\n",
    "    y_train = train_df['body_mass'].astype(float).values\n",
    "\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    # feature importance\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    model = lgb.LGBMRegressor(seed = SEED)\n",
    "    model.fit(X_train, y_train, verbose=-1)\n",
    "\n",
    "    feature_imp = pd.DataFrame(zip(X_train.columns,\n",
    "                                   model.feature_importances_.astype(float)), \n",
    "                               columns=['feature','imp']).sort_values(by='imp')\n",
    "\n",
    "    reduced_var = list(feature_imp.feature[feature_imp.imp>1])\n",
    "\n",
    "    X_train_new = X_train[reduced_var]\n",
    "    X_test_new  = X_test [reduced_var]\n",
    "\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    # modelling\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    n_fold = 5\n",
    "    sf = StratifiedKFold(n_fold, shuffle=True, random_state=SEED)\n",
    "\n",
    "    y_tr = []\n",
    "    y_te = []\n",
    "\n",
    "    c = 1\n",
    "    for tr_idx, val_idx in sf.split(X_train_new, y_train):\n",
    "\n",
    "        tr_nunique = X_train_new.iloc[tr_idx ]\\\n",
    "        [[col for col in X_train_new.columns if COL_TYPE[col]==str]]\\\n",
    "        .apply(lambda x:x.nunique()).values\n",
    "\n",
    "        va_nunique = X_train_new.iloc[val_idx]\\\n",
    "        [[col for col in X_train_new.columns if COL_TYPE[col]==str]]\\\n",
    "        .apply(lambda x:x.nunique()).values\n",
    "\n",
    "        match_nunique = np.array([col for col in X_train_new.columns if COL_TYPE[col]==str])[[tr_nunique != va_nunique]]\n",
    "\n",
    "        if len(match_nunique)>0:\n",
    "            for col in match_nunique:\n",
    "                del X_train_new[col]\n",
    "\n",
    "        print('#'*25, f'CV {c}')\n",
    "\n",
    "        #X_tr = onehot_encoding(X_train_new, COL_TYPE)\n",
    "        #X_te = onehot_encoding(X_test_new , COL_TYPE)\n",
    "\n",
    "        X_tr = X_train_new.copy()\n",
    "        X_te = X_test_new .copy()\n",
    "\n",
    "        model = build_lgb(x = X_tr.iloc[tr_idx ], \n",
    "                          y = y_train[tr_idx ], \n",
    "                          val_x = X_tr.iloc[val_idx], \n",
    "                          val_y = y_train[val_idx],\n",
    "                          init_points=INIT_POINTS, n_iter=N_ITER, cv=N_CV, \n",
    "                          ret_param=False, is_test=False, \n",
    "                          SEED=SEED,\n",
    "                          objective = OBJECTIVE)\n",
    "\n",
    "        y_tr_0 = model.predict(X_tr)\n",
    "        y_te_0 = model.predict(X_te)\n",
    "\n",
    "        y_tr.append(y_tr_0)\n",
    "        y_te.append(y_te_0)\n",
    "\n",
    "        c += 1\n",
    "\n",
    "    # seg별 predict값 넣기\n",
    "    train_df[seg_var_x+'_pred'] = np.median(y_tr,0)\n",
    "    test_df [seg_var_x+'_pred'] = np.median(y_te,0)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25288711",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T16:17:06.777237Z",
     "start_time": "2022-01-06T16:17:06.735732Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse_fn(train_df['cluster_pred'],train_df['body_mass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdec6f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T16:17:07.623844Z",
     "start_time": "2022-01-06T16:17:06.779737Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(train_df['body_mass'],train_df['cluster_pred'],hue=train_df['cluster'])\n",
    "plt.axline((train_df['body_mass'].min(),train_df['body_mass'].min()),\n",
    "           (train_df['body_mass'].max(),train_df['body_mass'].max()), \n",
    "           color='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f432528",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T16:18:39.809051Z",
     "start_time": "2022-01-06T16:18:39.800049Z"
    }
   },
   "outputs": [],
   "source": [
    "sub = test_df[['id','cluster_pred']].rename(columns={'cluster_pred':'Body Mass (g)'})\n",
    "sub\n",
    "\n",
    "sub.to_csv(OUT_PATH + 'out_lgbm_with_cluster.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9162907b",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Cluster별로 Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2de58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T16:20:42.410119Z",
     "start_time": "2022-01-06T16:19:44.241232Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_df = train12.copy()\n",
    "test_df  = test12 .copy()\n",
    "\n",
    "for seg_var_x in seg_var:\n",
    "\n",
    "    train_df[seg_var_x+'_pred']   = np.nan\n",
    "    test_df [seg_var_x+'_pred']   = np.nan\n",
    "\n",
    "    # segment별로 modelling\n",
    "    for iter in range(0,len(train_df[seg_var_x].value_counts().index)):\n",
    "\n",
    "        # segment setting\n",
    "        seg_var_value = train_df[seg_var_x].value_counts().index[iter]\n",
    "\n",
    "        # data setting\n",
    "        tr_seg_df = train_df[train_df[seg_var_x] == seg_var_value]\n",
    "        te_seg_df = test_df [test_df [seg_var_x] == seg_var_value]\n",
    "\n",
    "        drop_var = ['id','body_mass'] +\\\n",
    "        [seg_var_x           for seg_var_x in seg_var] +\\\n",
    "        [seg_var_x+'_pred'   for seg_var_x in seg_var] +\\\n",
    "        [seg_var_x+'_tr_idx' for seg_var_x in seg_var]\n",
    "\n",
    "        X_train = tr_seg_df[list(set(tr_seg_df.columns)-set(drop_var))]\n",
    "        X_test  = te_seg_df[list(set(te_seg_df.columns)-set(drop_var))]\n",
    "\n",
    "        y_train = tr_seg_df['body_mass'][tr_seg_df[seg_var_x] == seg_var_value].astype(float).values\n",
    "\n",
    "            \n",
    "        # feature importance\n",
    "        model = lgb.LGBMRegressor(seed = SEED)\n",
    "        model.fit(X_train, y_train, verbose=-1)\n",
    "\n",
    "        feature_imp = pd.DataFrame(zip(X_train.columns,\n",
    "                                       model.feature_importances_.astype(float)), \n",
    "                                   columns=['feature','imp']).sort_values(by='imp')\n",
    "            \n",
    "        reduced_var = list(feature_imp.feature[feature_imp.imp>1])\n",
    "\n",
    "        X_train_new = X_train[reduced_var]\n",
    "        X_test_new  = X_test [reduced_var]\n",
    "        \n",
    "        # modelling\n",
    "        n_fold = 4\n",
    "        sf = StratifiedKFold(n_fold, shuffle=True, random_state=SEED)\n",
    "\n",
    "        y_tr = []\n",
    "        y_te = []\n",
    "\n",
    "        c = 1\n",
    "        for tr_idx, val_idx in sf.split(X_train_new, y_train):\n",
    "\n",
    "            tr_nunique = X_train_new.iloc[tr_idx ]\\\n",
    "            [[col for col in X_train_new.columns if COL_TYPE[col]==str]]\\\n",
    "            .apply(lambda x:x.nunique()).values\n",
    "            \n",
    "            va_nunique = X_train_new.iloc[val_idx]\\\n",
    "            [[col for col in X_train_new.columns if COL_TYPE[col]==str]]\\\n",
    "            .apply(lambda x:x.nunique()).values\n",
    "\n",
    "            match_nunique = np.array([col for col in X_train_new.columns if COL_TYPE[col]==str])[[tr_nunique != va_nunique]]\n",
    "            \n",
    "            if len(match_nunique)>0:\n",
    "                for col in match_nunique:\n",
    "                    del X_train_new[col], X_test_new[col]\n",
    "                \n",
    "            print('#'*25, f'CV {c}')\n",
    "            \n",
    "            X_tr = X_train_new.copy()\n",
    "            X_te = X_test_new .copy()\n",
    "            \n",
    "            model = build_lgb(x = X_tr.iloc[tr_idx ], \n",
    "                              y = y_train[tr_idx ], \n",
    "                              val_x = X_tr.iloc[val_idx], \n",
    "                              val_y = y_train[val_idx],\n",
    "                              init_points=INIT_POINTS, n_iter=N_ITER, cv=N_CV, \n",
    "                              ret_param=False, is_test=False, \n",
    "                              SEED=SEED)\n",
    "\n",
    "            y_tr_0 = model.predict(X_tr)\n",
    "            y_te_0 = model.predict(X_te)\n",
    "\n",
    "            y_tr.append(y_tr_0)\n",
    "            y_te.append(y_te_0)\n",
    "\n",
    "            c += 1\n",
    "\n",
    "        # seg별 predict값 넣기\n",
    "        train_df[seg_var_x+'_pred'][train_df[seg_var_x] == seg_var_value] = np.median(y_tr,0) # np.where(np.mean(y_tr, 0)>0.5, 1, 0)\n",
    "        test_df [seg_var_x+'_pred'][test_df [seg_var_x] == seg_var_value] = np.median(y_te,0) # np.where(np.mean(y_te, 0)>0.5, 1, 0)\n",
    "        \n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf608a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T16:20:42.449124Z",
     "start_time": "2022-01-06T16:20:42.412119Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse_fn(train_df['cluster_pred'],train_df['body_mass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b635606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T16:20:43.330736Z",
     "start_time": "2022-01-06T16:20:42.451124Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(train_df['body_mass'],train_df['cluster_pred'],hue=train_df['cluster'])\n",
    "plt.axline((train_df['body_mass'].min(),train_df['body_mass'].min()),\n",
    "           (train_df['body_mass'].max(),train_df['body_mass'].max()), \n",
    "           color='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a98d3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-06T16:20:43.340737Z",
     "start_time": "2022-01-06T16:20:43.332736Z"
    }
   },
   "outputs": [],
   "source": [
    "sub = test_df[['id','cluster_pred']].rename(columns={'cluster_pred':'Body Mass (g)'})\n",
    "sub\n",
    "\n",
    "sub.to_csv(OUT_PATH + 'out_lgbm_segby_cluster.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7f9e2b",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### seg별 confusion matrix and f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f39ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seg_var_x in seg_var:\n",
    "    \n",
    "    tr_pred = train_df[f'{seg_var_x}_pred']\n",
    "    tr_true = train_df.target.astype(int).values\n",
    "    tr_f1   = f1_score(tr_pred,tr_true)\n",
    "    \n",
    "    print('-'*50)\n",
    "    print(f'{seg_var_x} - f1_score : {tr_f1:.2f}')\n",
    "    print(pd.crosstab(tr_pred,tr_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d1e05c",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### train_df 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57b0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv(OUT_PATH + 'train_df(2).csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e3479",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ef8f4",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### 각 seg별 f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_list = []\n",
    "for seg_var_x in seg_var:\n",
    "    f1 = f1_score(train_df[[seg_var_x + '_pred']].astype(int).values,train_df.target.astype(int).values)\n",
    "    n_blank = ' '*(max_char_len-len(seg_var_x))\n",
    "    \n",
    "    if seg_var_x==seg_var[0]: print(' '*max_char_len, 'f1_score')\n",
    "    print(f'{seg_var_x} {n_blank} {f1: .3f}')\n",
    "    \n",
    "    f1_score_list.append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c446a4",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### 각 segment의 weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0215155",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [f/sum(f1_score_list) for f in f1_score_list]\n",
    "for seg,w in zip(seg_var,weight):\n",
    "    n_blank = ' '*(max_char_len-len(seg))\n",
    "    if seg==seg_var[0]: print(' '*max_char_len, '  weight')\n",
    "    print(f'{seg} {n_blank} {w*100: .1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e9417b",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### weighted predicted value : [1,0]의 조합이라서 똑같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4c45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) mixed\n",
    "tr_pred_mix = train_df[[seg + '_pred' for seg in seg_var]].sum(axis=1)/len(seg_var)\n",
    "tr_f1_score_mix = f1_score(np.where(tr_pred_mix>0.5,1,0), train_df.target.astype(int).values)\n",
    "\n",
    "# (2) weighted mixed\n",
    "tr_pred_weighted_mix = np.array([train_df[seg+'_pred'].astype(int).values*weight[iter] \n",
    "                                 for iter,seg in enumerate(['is_ca', 'is_cp', 'sex', 'exang', 'slope2'])]).sum(axis=0)\n",
    "tr_f1_score_weighted_mix = f1_score(np.where(tr_pred_weighted_mix>0.5,1,0), train_df.target.astype(int).values)\n",
    "\n",
    "print(f'f1_score of          mixed segment predicted value:{tr_f1_score_mix         : .3f}')\n",
    "print(f'f1_score of weighted mixed segment predicted value:{tr_f1_score_weighted_mix: .3f}')\n",
    "\n",
    "# a=pd.DataFrame({'x' : np.where(tr_pred_mix>0.5,1,0),\n",
    "#                 'y' : np.where(tr_pred_weighted_mix>0.5,1,0)})\n",
    "# pd.crosstab(a.x,a.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8bbad9",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### seg들을 조합해서 pred 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ed81fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tr_pred_mix'] = np.where(train_df[[seg + '_pred' for seg in seg_var]].sum(axis=1)/len(seg_var)>0.5, '1', '0')\n",
    "test_df ['te_pred_mix'] = np.where(test_df [[seg + '_pred' for seg in seg_var]].sum(axis=1)/len(seg_var)>0.5, '1', '0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df359e05",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### confusion matrix and f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81dafb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(train_df['tr_pred_mix'].astype(int).values,train.target.astype(int).values)\n",
    "print(f'f1_score : {f1:.3f}\\n')\n",
    "print(pd.crosstab(train_df['tr_pred_mix'],train.target))\n",
    "\n",
    "' / '.join([f'{var}:{eval(var)}' for var in ini_var])\n",
    "\n",
    "\n",
    "# f1 : 0.899 ('SEED:777 / INIT_POINTS:15 / N_ITER:15 / N_CV:4 / EARLY_STOPPING_ROUNDS:30 / N_ESTIMATORS:2000 / METRIC:binary_logloss / PLOT:False / SCALE:True / INTERACTION:True')\n",
    "# f1 : 0.859 ('SEED:777 / INIT_POINTS:15 / N_ITER:15 / N_CV:4 / EARLY_STOPPING_ROUNDS:30 / N_ESTIMATORS:2000 / METRIC:binary_logloss / PLOT:False / SCALE:True / INTERACTION:False')\n",
    "\n",
    "# f1 : 0.897 ('SEED:777 / INIT_POINTS:15 / N_ITER:15 / N_CV:4 / EARLY_STOPPING_ROUNDS:30 / N_ESTIMATORS:2000 / METRIC:binary_logloss / PLOT:False / SCALE:False / INTERACTION:True')\n",
    "# f1 : 0.877 ('SEED:777 / INIT_POINTS:15 / N_ITER:15 / N_CV:4 / EARLY_STOPPING_ROUNDS:30 / N_ESTIMATORS:2000 / METRIC:binary_logloss / PLOT:False / SCALE:False / INTERACTION:False')\n",
    "\n",
    "# f1 : 0.920 ('SEED:777 / INIT_POINTS:45 / N_ITER:100 / N_CV:4 / EARLY_STOPPING_ROUNDS:30 / N_ESTIMATORS:2000 / METRIC:binary_logloss / PLOT:False / SCALE:True / INTERACTION:True')\n",
    "# f1 : 0.860 ('SEED:777 / INIT_POINTS:50 / N_ITER:200 / N_CV:4 / EARLY_STOPPING_ROUNDS:30 / N_ESTIMATORS:2000 / METRIC:auc / PLOT:False / SCALE:True / INTERACTION:True')\n",
    "\n",
    "# f1 : 0.907 ('SEED:777 / INIT_POINTS:50 / N_ITER:300 / N_CV:4 / EARLY_STOPPING_ROUNDS:30 / N_ESTIMATORS:2000 / METRIC:binary_logloss / PLOT:False / SCALE:True / INTERACTION:True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba92e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['te_pred_mix'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429901fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['target'] = ['1' if pred=='1' else '0' for pred in test_df['te_pred_mix']]\n",
    "sub.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339ba370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 점수 : 0.92537\n",
    "# sub.to_csv(OUT_PATH + 'sample_18.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da74baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.read_csv(OUT_PATH + '★sample_9_mixed.csv')\n",
    "b=sub\n",
    "\n",
    "print(pd.crosstab(a.target,b.target))\n",
    "print(f1_score(a.target.astype(int).values,b.target.astype(int).values))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "847px",
    "left": "0px",
    "top": "91px",
    "width": "360px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
